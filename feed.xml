<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<link href="https://matklad.github.io/feed.xml" rel="self" type="application/atom+xml"/>
<link href="https://matklad.github.io" rel="alternate" type="text/html"/>
<updated>2024-11-24T00:33:48.236Z</updated>
<id>https://matklad.github.io/feed.xml</id>
<title type="html">matklad</title>
<subtitle>Yet another programming blog by Alex Kladov aka matklad.</subtitle>
<author><name>Alex Kladov</name></author>

<entry>
<title type="text">SemVer Is Not About You</title>
<link href="https://matklad.github.io/2024/11/23/semver-is-not-about-you.html" rel="alternate" type="text/html" title="SemVer Is Not About You" />
<published>2024-11-23T00:00:00+00:00</published>
<updated>2024-11-23T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/11/23/semver-is-not-about-you</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[A popular genre of articles for the past few year has been a SemVer Critique, pointing out various
things that are wrong with SemVer itself, or with the way SemVer is being applied, and, customary,
suggesting an alternative versioning scheme. Usually, the focus is either on how SemVer ought to be
used, by library authors (nitpicking the definition of a breaking change), or on how SemVer is (not)
useful for a library consumer (nitpicking the definition of a breaking change).]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/11/23/semver-is-not-about-you.html"><![CDATA[
<h1><span>SemVer Is Not About You</span> <time class="meta" datetime="2024-11-23">Nov 23, 2024</time></h1>
<p><span>A popular genre of articles for the past few year has been a </span>&ldquo;<span>SemVer Critique</span>&rdquo;<span>, pointing out various</span>
<span>things that are wrong with SemVer itself, or with the way SemVer is being applied, and, customary,</span>
<span>suggesting an alternative versioning scheme. Usually, the focus is either on how SemVer ought to be</span>
<span>used, by library authors (nitpicking the definition of a breaking change), or on how SemVer is (not)</span>
<span>useful for a library consumer (nitpicking the definition of a breaking change).</span></p>
<p><span>I think these are valid lenses to study SemVer through, but not the most useful. This article</span>
<span>suggest an alternative framing: </span><span class="display"><span>SemVer is not about you.</span></span></p>
<p><span>Before we begin, I would like to carefully delineate the scope. Among other things, SemVer can be</span>
<span>applied to, broadly speaking, applications and libraries. Applications are stand-alone software</span>
<span>artifacts, usable as is. Libraries exist within the larger library ecosystem, and are building</span>
<span>blocks for assembling applications. Libraries both depend on and are depended upon by  other</span>
<span>libraries. In the present article, we will look </span><em><span>only</span></em><span> at the library side.</span></p>
<hr>
<p><span>At the first glance, it appears that SemVer solves the problem of informing the user when to do the</span>
<span>upgrade: upgrade patch for latest bugfixes, upgrade minor if you want new features, upgrade major if</span>
<span>you want new features and are ready to clean-up your code. But this is not the primary value of this</span>
<span>versioning scheme. The </span><em><span>real</span></em><span> reason of semver is for managing transitive dependencies.</span></p>
<p><span>Let</span>&rsquo;<span>s say you are using some version of </span><code>apples</code><span> library and some version of </span><code>oranges</code><span> library. And</span>
<span>suppose they both depend on the </span><code>trees</code><span> library. Because </span><code>apples</code><span> and </span><code>oranges</code><span> were authored at</span>
<span>different times, they do not necessary depend on the </span><em><span>same</span></em><span> version of </span><code>trees</code><span>. There are two paths</span>
<span>from here.</span></p>
<p><span>The first is to include two different versions of </span><code>trees</code><span> library with your app. This is unfortunate</span>
<span>for a trivial reason of code bloat, and for a more subtle reason of interface leaking: if for some</span>
<span>reason your code needs to pass a </span><code>tree</code><span> originating in </span><code>apples</code><span> over to the </span><code>oranges</code><span>, you must use</span>
<span>exactly the same </span><code>trees</code><span> library.</span></p>
<p><span>The second path is to somehow unify transitive dependencies, and pick a single version of </span><code>trees</code>
<span>that</span>&rsquo;<span>s good for both </span><code>apples</code><span> and </span><code>oranges</code><span>. But perhaps there isn</span>&rsquo;<span>t a version that works for both?</span></p>
<p><span>Who</span>&rsquo;<span>s the right person to choose the appropriate course of action? It </span><em><span>could</span></em><span> be you, but that</span>&rsquo;<span>s</span>
<span>unfortunate </span>&mdash;<span> you are using libraries precisely because you want to avoid thinking too much about</span>
<span>their internals. You don</span>&rsquo;<span>t know how </span><code>apples</code><span> is using </span><code>trees</code><span>. You </span><em><span>could</span></em><span> learn that, but,</span>
<span>arguably, that</span>&rsquo;<span>s not a good tradeoff (if it is, perhaps you shouldn</span>&rsquo;<span>t depend on </span><code>apples</code><span> and instead</span>
<span>maintain your own). What</span>&rsquo;<span>s worse, for featurefull applications dependency trees run very deep,</span>
<span>potential for conflicts scales at least linearly, and there</span>&rsquo;<span>s only a single you.</span></p>
<p><span>Another candidate is the author of the </span><code>trees</code><span> library </span>&mdash;<span> they don</span>&rsquo;<span>t know </span><code>apples</code><span> and </span><code>oranges</code>
<span>directly, but they should be thinking about how their library </span><em><span>could</span></em><span> be used. </span><em><span>And</span></em><span>, because</span>
<span>different libraries tend to have different authors, the work for resolving version conflicts get</span>
<span>distributed across the set of people that also scales linearly!</span></p>
<p><em><span>This</span></em><span> is the problem that SemVer solves </span>&mdash;<span> it has nothing to do with your code or your direct</span>
<span>dependencies, it</span>&rsquo;<span>s all about dependencies of your dependencies. SemVer </span><em><span>is</span></em><span> library maintainer</span>
<span>saying when two versions of their library can be unified:</span></p>
<ul>
<li>
<span>If major version is bumped, no unification happens, the library will get duplicated.</span>
</li>
<li>
<span>If major is not bumped, the versions can be unified.</span>
</li>
</ul>
<p><span>That</span>&rsquo;<span>s it! That</span>&rsquo;<span>s the whole thing! All the talk about breaking changes is downstream of this actual</span>
<span>behavior of version resolvers.</span></p>
<hr>
<p><span>Notably, if you are a library maintainer, SemVer isn</span>&rsquo;<span>t about you either. When deciding between major</span>
<span>and minor, you shouldn</span>&rsquo;<span>t be thinking about your </span><em><span>direct</span></em><span> dependents. They knowingly use your</span>
<span>library, so they are capable of making informed decisions and will manage just fine. The problem are</span>
<span>your transitive dependents. If you release a new major version, dependencies of some application up</span>
<span>the stack could get wedged if somewhere in its dependencies tree there are both versions of your</span>
<span>library which need interoperable types.</span></p>
<p><span>Or, rather, if you release a new major version, it is guaranteed that some application would have</span>
<span>two copies of your library. There</span>&rsquo;<span>s no such thing as atomic upgrade of dependencies across the</span>
<span>ecosystem, propagating your new major will take time and there will be extended period where</span>
<span>both majors are used, by different libraries, and both majors end up in applications</span>&rsquo;
<span>lockfiles. The question is rather would this be more harmful than just code bloat? If your library</span>
<span>ends up in other</span>&rsquo;<span>s public API you will likely lock some upstream applications in a variant of the</span>
<span>following problem:</span></p>
<ul>
<li>
<span>We need to update </span><code>lemons</code><span> to new version to get access to this critical bug fix for the new MacOS</span>
<span>version</span>
</li>
<li>
<span>But </span><code>lemons</code><span> is an actively developed library, it upgraded to the new version of </span><code>trees</code><span> library</span>
<span>three months ago and MacOS bugfix sits on top of that version.</span>
</li>
<li>
<span>But we also use </span><code>limes</code><span>, which is a bit of a more niche product, and so hasn</span>&rsquo;<span>t seen upgrade for</span>
<span>about a year.</span>
</li>
<li>
<span>And we also use the same pool of </span><code>trees</code><span> for both, so our latest </span><code>limes</code><span> prevent upgrading</span>
<code>lemons</code><span>.</span>
</li>
</ul>
<p><span>It</span>&rsquo;<span>s also worth thinking about virility of major versions </span>&mdash;<span> if your library is someone elses</span>
<span>public API, </span><em><span>your</span></em><span> major bump implies </span><em><span>their</span></em><span> major bump, which is of course bad because putting</span>
<span>work on the plate of other maintainers is bad, but, what</span>&rsquo;<span>s worse, is that this virally amplifies the</span>
<span>number of unsatisfiable graph of dependencies a-la the example above.</span></p>
<section id="SemVer">

    <h2>
    <a href="#SemVer"><span>SemVer-</span><span>-</span> </a>
    </h2>
<p><span>I</span>&rsquo;<span>ve seen two interesting extensions to the core SemVer. One is the observation that, to make</span>
<span>tooling work, only two version numbers are sufficient. There</span>&rsquo;<span>s no </span><em><span>real</span></em><span> difference between </span><code>patch</code>
<span>and </span><code>minor</code><span>, as far as the actual behavior of version resolution algorithm goes. I am sympathetic to</span>
<span>this argument!</span></p>
<p><span>The second one is an observation that many projects follow the </span>&ldquo;<span>deprecate than remove cycle</span>&rdquo;<span>. I</span>&rsquo;<span>ve</span>
<span>learned this with the release of Ember 2.0. The big deal about Ember 2.0 is that the </span><em><span>only</span></em><span> thing</span>
<span>that it did was the removal of deprecation warnings. Code that didn</span>&rsquo;<span>t emit warnings on the latest</span>
<span>Ember 1.x was compatible with 2.0.</span></p>
<p><span>This feels like the fundamentally right way going about the larger, more important building blocks.</span>
<span>And you sort-of can do this with semver today, if you declare that you are compatible with </span><code>"1.9,
2.0"</code><span>. But, even today, many years after Ember 2.0, this still feels like a cute trick. This isn</span>&rsquo;<span>t</span>
<span>yet a pattern with a catchy name (like release trains or not rocket science rule) that everyone is</span>
<span>using because it is an obviously good idea</span></p>
</section>
<section id="And-Now-To-Something-Completely-Different">

    <h2>
    <a href="#And-Now-To-Something-Completely-Different"><span>And Now To Something Completely Different</span> </a>
    </h2>
<p><span>Circling back to the introduction, the general pattern here is that there</span>&rsquo;<span>s a prescriptivist</span>
<span>approach and a descriptivist one. Prescriptivist argues about the right and wrong ways to use a</span>
<span>particular tool. Descriptivist avoids value judgement, and describes how the thing actually behaves.</span></p>
<p><span>Another instance of this pattern playing out I</span>&rsquo;<span>ve noticed are log levels. You can get very</span>
<span>philosophical about the difference between </span><code>error</code><span>, </span><code>warn</code><span> and </span><code>info</code><span>. But what helps is looking at</span>
<span>what they do:</span></p>
<ul>
<li>
<code>error</code><span> pages the operator immediately.</span>
</li>
<li>
<code>warn</code><span> pages if it repeats frequently.</span>
</li>
<li>
<code>info</code><span> is what you see in the prog logs when you actively look at them.</span>
</li>
<li>
<span>And </span><code>debug</code><span> is what your developers see when they enable extra logging.</span>
</li>
</ul>

<figure class="blockquote">
<blockquote><p><span>давайте одевать одежду</span><br>
<span>давайте звонит говорить</span><br>
<span>а на прескриптивистов будем</span><br>
<span>ложить</span></p>
</blockquote>
<figcaption><cite><a href="https://avva.livejournal.com/2748021.html"><span>avva</span></a></cite></figcaption>
</figure>
</section>
]]></content>
</entry>

<entry>
<title type="text">A Missing IDE Feature</title>
<link href="https://matklad.github.io/2024/10/14/missing-ide-feature.html" rel="alternate" type="text/html" title="A Missing IDE Feature" />
<published>2024-10-14T00:00:00+00:00</published>
<updated>2024-10-14T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/10/14/missing-ide-feature</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[Slightly unusual genre --- with this article, I want to try to enact a change in the world. I
believe that there is a missing IDE feature which is:]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/10/14/missing-ide-feature.html"><![CDATA[
<h1><span>A Missing IDE Feature</span> <time class="meta" datetime="2024-10-14">Oct 14, 2024</time></h1>
<p><span>Slightly unusual genre </span>&mdash;<span> with this article, I want to try to enact a change in the world. I</span>
<span>believe that there is a </span>&ldquo;<span>missing</span>&rdquo;<span> IDE feature which is:</span></p>
<ul>
<li>
<span>very easy to implement (these days),</span>
</li>
<li>
<span>is a large force multiplier for experienced users,</span>
</li>
<li>
<span>is conspicuously missing from almost every editor.</span>
</li>
</ul>
<p><span>The target audience here is anyone who can land a PR in Zed, VS Code, Helix, Neovim, Emacs, Kakoune,</span>
<span>or any other editor or any language server. The blog post would be a success if one of you feels</span>
<span>sufficiently inspired to do the thing!</span></p>
<section id="The-Feature">

    <h2>
    <a href="#The-Feature"><span>The Feature</span> </a>
    </h2>
<p><span>Suppose you are casually reading the source code of rust-analyzer, and are curious about handling of</span>
<span>method bodies. There</span>&rsquo;<span>s a </span><code>Body</code><span> struct in the code base, and you want to understand how it is used.</span></p>
<p><span>Would you rather look at this?</span></p>

<figure>

<img alt="" src="https://github.com/user-attachments/assets/4c794726-1b56-4c32-b51e-e0b671b73bab">
</figure>
<p><span>Or this?</span></p>

<figure>

<img alt="" src="https://github.com/user-attachments/assets/e0d9adf5-15a1-48e1-a1b8-3762104a031e">
</figure>
<p><span>(The screenshots are from IntelliJ/RustRover, because of course it gets this right)</span></p>
<p><span>The second option is clearly superior </span>&mdash;<span> it conveys significantly more useful information in the</span>
<span>same amount of pixels. Function names, argument lists and return types are so much more valuable</span>
<span>than a body of any particular function. Especially if the function is a page-full of boilerplate</span>
<span>code!</span></p>
<p><span>And this is the feature I am asking for </span>&mdash;<span> make the code look like the second image. Or,</span>
<span>specifically, </span><span class="display"><strong><span>Fold Method Bodies by Default</span></strong><span>.</span></span></p>
<p><span>There are two components here. </span><em><span>First</span></em><span>, only method bodies are folded. This is a syntactic check </span>&mdash;
<span>we are </span><em><span>not</span></em><span> folding the second level. For code like</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">f</span>() { ... }</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">S</span> {</span>
<span class="line">    <span class="hl-keyword">fn</span> <span class="hl-title function_">g</span>(&amp;<span class="hl-keyword">self</span>) { ... }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Both </span><code>f</code><span> and </span><code>g</code><span> are folded, but </span><code>impl S</code><span> is not. Similarly, function parameters and function body</span>
<span>are actually on the same level of folding hierarchy, but it is imperative that parameters are </span><em><span>not</span></em>
<span>folded. This is the part that was hard ten years ago but is easy today. </span>&ldquo;<span>what is function body</span>&rdquo;<span> is a</span>
<span>non-trivial question, which requires proper parsing of the code. These days, either an LSP server or</span>
<span>Tree-sitter can answer this question quickly and reliably.</span></p>
<p><em><span>The second</span></em><span> component of the feature is that folded is a default state. It is not a </span>&ldquo;<span>fold method</span>
<span>bodies</span>&rdquo;<span> </span><em><span>action</span></em><span>. It is a setting that ensures that, whenever you visit a new file, bodies are</span>
<span>folded by default. To make this work, the editor should be smart to seamlessly unfold specific</span>
<span>function when appropriate. For example, if you </span>&ldquo;<span>go to definition</span>&rdquo;<span> to a function, that function</span>
<span>should get unfolded, while the surrounding code should remail folded.</span></p>
<p><span>Now that I have explained how the feature works, I will </span><em><span>not</span></em><span> try to motivate it. I think it is</span>
<span>pretty obvious how awesome this actually is. Code is read more often than written, and this is one</span>
<span>of the best multipliers for readability. </span><em><span>Most</span></em><span> of the code is in method bodies, but most important</span>
<span>code is in function signatures. Folding bodies auto-magically hide the 80% of boring code, leaving</span>
<span>the most important 20%. It was in 2018 when I last used an IDE (IntelliJ) which has this implemented</span>
<span>properly, and I</span>&rsquo;<span>ve been missing this function ever since!</span></p>
<p><span>You might also be wondering whether it is the same feature as the Outline, that special UI which</span>
<span>shows a graphical, hierarchical table of contents of the file. It is true that outline and</span>
<span>fold-bodies-by-default attack the same issue. But I</span>&rsquo;<span>d argue that folding solves it better. This is</span>
<span>an instance of a common pattern. In a smart editor, it is often possible to implement any given</span>
<span>feature either by </span>&ldquo;<span>lowering</span>&rdquo;<span> it to plain text, or by creating a dedicated GUI. And the lowering</span>
<span>approach almost always wins, because it gets to re-use all existing functionality for free. For</span>
<span>example, the folding approach trivially gives you an ability to move a bunch of functions from one</span>
<span>impl block to the other by selecting them with </span><kbd><kbd>Shift </kbd>+<kbd> Down</kbd></kbd><span>, cutting with </span><kbd><kbd>Ctrl </kbd>+<kbd> X</kbd></kbd>
<span>and pasting with </span><kbd><kbd>Ctrl </kbd>+<kbd> V</kbd></kbd><span>.</span></p>
</section>
<section id="Call-to-Action">

    <h2>
    <a href="#Call-to-Action"><span>Call to Action</span> </a>
    </h2>
<p><span>So, if you are a committer to one of the editors, please consider adding a </span>&ldquo;<span>fold function bodies by</span>
<span>default</span>&rdquo;<span> mode. It probably should be off by default, as it can easily scare new users away, but it</span>
<span>should be there for power users to enable, and it should be prominently documented, so that people</span>
<span>can learn that they want it. After the checkbox is in place, see if you can implement the actual</span>
<span>logic! If your editor uses Tree-sitter, this should be relatively easy </span>&mdash;<span> its syntax tree contains</span>
<span>all the information you need. Just make sure that:</span></p>
<ul>
<li>
<span>bodies are folded when the new file is opened,</span>
</li>
<li>
<span>the editor unfolds them when appropriate (generally, when navigated to a function from elsewhere).</span>
</li>
</ul>
<p><span>If your editor is </span><em><span>not</span></em><span> based on Tree-sitter, you</span>&rsquo;<span>ll have a harder time. In theory, the information</span>
<span>should be readily available from the language server, but LSP currently doesn</span>&rsquo;<span>t expose it. Here</span>&rsquo;<span>s</span>
<span>the problem:</span></p>
<p><a href="https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#foldingRangeKind" class="url">https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#foldingRangeKind</a></p>
<p><span>There</span>&rsquo;<span>s no </span><code>body</code><span> kind there! Adding it should be trivially technically, but it always is a pain to</span>
<span>get something into the protocol if you are not VS Code.</span></p>
</section>
<section id="My-Role">

    <h2>
    <a href="#My-Role"><span>My Role</span> </a>
    </h2>
<p><span>What is my job here, besides sitting there and writing blog posts? I actually think that writing</span>
<span>this down is quite valuable!</span></p>
<p><span>I suppose the feature is still commonly missing due to a two-sided market failure </span>&mdash;<span> the feature</span>
<span>doesn</span>&rsquo;<span>t exist, so prospective users don</span>&rsquo;<span>t realize that it is possible, and don</span>&rsquo;<span>t ask editor</span>&rsquo;<span>s</span>
<span>authors to implement it. Without users asking, editor authors themselves don</span>&rsquo;<span>t realize this feature</span>
<span>could exist, and don</span>&rsquo;<span>t rush implementing it. This is exacerbated by the fact that it </span><em><span>was</span></em><span> a hard</span>
<span>feature to implement ten years ago, when we didn</span>&rsquo;<span>t have Tree-sitter/LSP, so there are poor</span>
<span>workarounds in place </span>&mdash;<span> actions to fold a certain </span><em><span>level</span></em><span>. These workarounds the prevent the proper</span>
<span>feature from gaining momentum.</span></p>
<p><span>So here I hope to maybe tip the equilibrium</span>&rsquo;<span>s scale a bit, and start a feedback loop where more</span>
<span>people realize that they </span><em><span>want</span></em><span> this feature, such that it is implemented in some of the more</span>
<span>experimental editors, which hopefully would expose the feature to more users, popularizing it until</span>
<span>it gets implemented everywhere!</span></p>
<p><span>Still, just talking </span><em><span>isn</span>&rsquo;<span>t</span></em><span> everything I did here! Six years ago, I implemented the language-server</span>
<span>side of this in rust-analyzer:</span></p>
<p><a href="https://github.com/rust-lang/rust-analyzer/commit/23b040962ff299feeef1f967bc2d5ba92b01c2bc" class="url">https://github.com/rust-lang/rust-analyzer/commit/23b040962ff299feeef1f967bc2d5ba92b01c2bc</a></p>
<p><span>This currently </span><em><span>isn</span>&rsquo;<span>t</span></em><span> exposed to LSP, because it doesn</span>&rsquo;<span>t allow flagging a folding range as method</span>
<span>body. To fix </span><em><span>that</span></em><span> I opened this VS Code issue (LSP generally avoids doing something before VS</span>
<span>Code):</span></p>
<p><a href="https://github.com/microsoft/vscode/issues/128912" class="url">https://github.com/microsoft/vscode/issues/128912</a></p>
<p><span>And since then I have been quietly waiting for some editor (not necessary VS Code) to pick this up.</span>
<span>This hasn</span>&rsquo;<span>t happened yet, hence this article!</span></p>
<p><span>Thanks for reading!</span></p>
</section>
]]></content>
</entry>

<entry>
<title type="text">Two Workflow Tips</title>
<link href="https://matklad.github.io/2024/10/08/two-tips.html" rel="alternate" type="text/html" title="Two Workflow Tips" />
<published>2024-10-08T00:00:00+00:00</published>
<updated>2024-10-08T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/10/08/two-tips</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[An article about a couple of relatively recent additions to my workflow which I wish I knew about
years ago.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/10/08/two-tips.html"><![CDATA[
<h1><span>Two Workflow Tips</span> <time class="meta" datetime="2024-10-08">Oct 8, 2024</time></h1>
<p><span>An article about a couple of relatively recent additions to my workflow which I wish I knew about</span>
<span>years ago.</span></p>
<section id="Split-And-Go-To-Definition">

    <h2>
    <a href="#Split-And-Go-To-Definition"><span>Split And Go To Definition</span> </a>
    </h2>
<p><span>Go to definition is super useful (in general, navigation is much more important than code</span>
<span>completion). But often, when I use </span>&ldquo;<span>goto def</span>&rdquo;<span> I don</span>&rsquo;<span>t actually mean to permanently </span><em><span>go</span></em><span> there.</span>
<span>Rather, I want to stay where I am, but I need a bit more context about a particular thing at point.</span></p>
<p><span>What I</span>&rsquo;<span>ve found works really great in this context is to </span><em><span>split</span></em><span> the screen in two, and issue </span>&ldquo;<span>go to</span>
<span>def</span>&rdquo;<span> in the split. So that you see both the original context, and the definition at the same time,</span>
<span>and can choose just how far you would like to go. Here</span>&rsquo;<span>s an example, where I want to understand how</span>
<code>apply</code><span> function works, and, to understand </span><em><span>that</span></em><span>, I want to quickly look up the definition of</span>
<code>FuzzOp</code><span>:</span></p>

<figure>

<video src="https://github.com/user-attachments/assets/0adc0f3c-ec7e-49a6-9e7e-f93f30704fdd" controls muted=true></video>
</figure>
<p><span>VS Code actually has a first-class UI for something like this, called </span>&ldquo;<span>Peek Definition</span>&rdquo;<span> but it is</span>
<span>just not good </span>&mdash;<span> it opens some kind of separate pop-up, with a completely custom UX. It</span>&rsquo;<span>s much more</span>
<span>fruitful to compose two existing basic features </span>&mdash;<span> splitting the screen and going to definition.</span></p>
<p><span>Note that in the example above I do move focus to the split. I also tried a version that keeps focus</span>
<span>in the original split, but focusing new one turned out to be much better. You actually don</span>&rsquo;<span>t always</span>
<span>know up front which split would become the </span>&ldquo;<span>main</span>&rdquo;<span> one, and moving the focus gives you flexibility of</span>
<span>moving around, closing the split, or closing the other split.</span></p>
<p><span>I highly recommend adding a shortcut for this action. It</span>&rsquo;<span>s a good idea to make it a </span>&ldquo;<span>complementary</span>&rdquo;
<span>shortcut for the usual goto definition. I use </span><kbd><kbd>, .</kbd></kbd><span> for goto definition, and hence </span><kbd><kbd>, ></kbd></kbd>
<span>is the splitting version:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-punctuation">{</span> <span class="hl-attr">&quot;key&quot;</span><span class="hl-punctuation">:</span> <span class="hl-string">&quot;, .&quot;</span><span class="hl-punctuation">,</span>       <span class="hl-attr">&quot;command&quot;</span><span class="hl-punctuation">:</span> <span class="hl-string">&quot;editor.action.revealDefinition&quot;</span> <span class="hl-punctuation">}</span><span class="hl-punctuation">,</span></span>
<span class="line"><span class="hl-punctuation">{</span> <span class="hl-attr">&quot;key&quot;</span><span class="hl-punctuation">:</span> <span class="hl-string">&quot;, shift+.&quot;</span><span class="hl-punctuation">,</span> <span class="hl-attr">&quot;command&quot;</span><span class="hl-punctuation">:</span> <span class="hl-string">&quot;editor.action.revealDefinitionAside&quot;</span> <span class="hl-punctuation">}</span><span class="hl-punctuation">,</span></span></code></pre>

</figure>
</section>
<section id="s-1">

    <h2>
    <a href="#s-1"><span>, .</span> </a>
    </h2>
<p><span>Yes, you are reading this right. </span><kbd><kbd>, .</kbd></kbd><span>, that is a comma followed by a full stop, is my goto</span>
<span>definition shortcut. This is not some kind of evil vim mode. I use pedestrian non-modal editing,</span>
<span>where I copy with </span><kbd><kbd>ctrl </kbd>+<kbd> c</kbd></kbd><span>, move to the beginning of line with </span><kbd><kbd>Home</kbd></kbd><span> and kill a word</span>
<span>with </span><kbd><kbd>ctrl </kbd>+<kbd> Backspace</kbd></kbd><span> (though keys like </span><kbd><kbd>Home</kbd></kbd><span>, </span><kbd><kbd>Backspace</kbd></kbd><span>, or arrows are on my</span>
<span>home row thanks to </span><a href="https://github.com/jtroo/kanata"><code>kanata</code></a><span>).</span></p>
<p><span>And yet, I use </span><code>,</code><span> as a first keypress in a sequence for multiple shortcuts. That is, </span><kbd><kbd>, .</kbd></kbd><span> is</span>
<span>not </span><kbd><kbd>, </kbd>+<kbd> .</kbd></kbd><span> pressed together, but rather a </span><kbd><kbd>,</kbd></kbd><span> followed by a separate </span><kbd><kbd>.</kbd></kbd><span>. So,</span>
<span>when I press </span><kbd><kbd>,</kbd></kbd><span> my editor doesn</span>&rsquo;<span>t actually type a comma, but rather waits for me to complete</span>
<span>the shortcut. I have many of them, with just a few being:</span></p>
<ul>
<li>
<kbd><kbd>, .</kbd></kbd><span> goes to definition,</span>
</li>
<li>
<kbd><kbd>, ></kbd></kbd><span> goes to definition in a split,</span>
</li>
<li>
<kbd><kbd>, r</kbd></kbd><span> runs a task,</span>
</li>
<li>
<kbd><kbd>, e s</kbd></kbd><span> </span><strong><span>e</span></strong><span>dits selection by sorting it, </span><kbd><kbd>, e C</kbd></kbd><span> converts to camel</span><strong><span>C</span></strong><span>ase,</span>
</li>
<li>
<kbd><kbd>, o g</kbd></kbd><span> </span><strong><span>o</span></strong><span>pens </span><a href="https://github.com/kahole/edamagit"><span>magit for VS Code</span></a><span>, </span><kbd><kbd>, o k</kbd></kbd><span> opens</span>
<span>keybindings.</span>
</li>
<li>
<kbd><kbd>, w</kbd></kbd><span> re-</span><strong><span>w</span></strong><span>raps selection at 80 (something I just did to format the previous bullet point),</span>
<kbd><kbd>, p</kbd></kbd><span> </span><strong><span>p</span></strong><span>retty-prints the whole file.</span>
</li>
</ul>
<p><span>I</span>&rsquo;<span>ve used many different shortcut schemes, but this is by far the most convenient one for me. How do</span>
<span>I type an comma? I bind </span><kbd><kbd>, Space</kbd></kbd><span> and </span><kbd><kbd>, Enter</kbd></kbd><span> to insert comma and a space/newline</span>
<span>respectively, which handles most of the cases. And there</span>&rsquo;<span>s </span><kbd><kbd>, ,</kbd></kbd><span> which types just a lone comma.</span></p>
<p><span>To remember longer sequences, I pair the comma with</span>
<a href="https://github.com/VSpaceCode/vscode-which-key"><span>whichkey</span></a><span>, such that, when I type </span><kbd><kbd>, e</kbd></kbd><span>, what</span>
<span>I see is actually a menu of editing operations:</span></p>

<figure>

<img alt="" src="https://github.com/user-attachments/assets/57f50d23-8c4d-4ec3-a1f0-51fce46bf4d0">
</figure>
<p><span>This horrible idea was born in the mind of Susam Pal, and is officially (and aptly I should say)</span>
<span>named </span><span class="display"><a href="https://susam.github.io/devil/"><span>Devil Mode</span></a><span>.</span></span></p>
<p><span>I highly recommend trying it out! It is the perfect interface for actions that you do once in a</span>
<span>while. Where it doesn</span>&rsquo;<span>t work is for actions you want to repeat. For example, if you want to cycle</span>
<span>through compilation errors, binding </span><kbd><kbd>, e</kbd></kbd><span> to the </span>&ldquo;<span>next error</span>&rdquo;<span> would probably be a bad</span>
<span>idea, as typing </span><code>, e , e , e</code><span> to cycle three times is quite tiring.</span></p>
<p><span>This is actually a common theme, there are </span><em><span>many</span></em><span> things you might to cycle back and forward</span>
<span>through:</span></p>
<ul>
<li>
<span>completion suggestions</span>
</li>
<li>
<span>compiler errors</span>
</li>
<li>
<span>textual search results</span>
</li>
<li>
<span>reference search results</span>
</li>
<li>
<span>merge conflicts</span>
</li>
<li>
<span>working tree changes</span>
</li>
</ul>
<p><span>It is mighty annoying to have to remember different shortcuts for all of them, isn</span>&rsquo;<span>t it? If only</span>
<span>there was some way to have a universal pair of shortcuts for the next/prev generalized motion</span>&hellip;</p>
<p><span>The insight here is that you</span>&rsquo;<span>d rarely need to cycle through several different categories of things</span>
<span>at the same time. So I bind the venerable </span><kbd><kbd>ctrl</kbd>+<kbd>n</kbd></kbd><span> and </span><kbd><kbd>ctrl</kbd>+<kbd>p</kbd></kbd><span> to </span><em><span>repeating</span></em><span> the last</span>
<span>next/prev motion. So, if the last next thing was a worktree change, then </span><kbd><kbd>ctrl</kbd>+<kbd>n</kbd></kbd><span> moves me to</span>
<span>the next worktree change. But if I then query the next compilation error, the subsequent</span>
<kbd><kbd>ctrl</kbd>+<kbd>n</kbd></kbd><span> would continue cycling through compilation errors. To kick-start the cycle, I have a</span>
<kbd><kbd>, n</kbd></kbd><span> hydra:</span></p>
<ul>
<li>
<kbd><kbd>, n e</kbd></kbd><span> next error</span>
</li>
<li>
<kbd><kbd>, n c</kbd></kbd><span> next change</span>
</li>
<li>
<kbd><kbd>, n C</kbd></kbd><span> next merge Conflict</span>
</li>
<li>
<kbd><kbd>, n r</kbd></kbd><span> next reference</span>
</li>
<li>
<kbd><kbd>, n f</kbd></kbd><span> next find</span>
</li>
<li>
<kbd><kbd>, n .</kbd></kbd><span> previous edit</span>
</li>
</ul>
<p><span>I don</span>&rsquo;<span>t know if there</span>&rsquo;<span>s some existing VS Code extension to do this, I implement this in</span>
<a href="https://github.com/matklad/config/blob/0f690f89c80b0e246909b54a0e97c67d5ce6ab0c/my-code/src/main.ts#L63-L96"><span>my personal extension</span></a><span>.</span></p>
<p><span>Hope this is useful! Now go and make a deal with the devil yourself!</span></p>
</section>
]]></content>
</entry>

<entry>
<title type="text">On Ousterhout's Dichotomy</title>
<link href="https://matklad.github.io/2024/10/06/ousterhouts-dichotomy.html" rel="alternate" type="text/html" title="On Ousterhout's Dichotomy" />
<published>2024-10-06T00:00:00+00:00</published>
<updated>2024-10-06T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/10/06/ousterhouts-dichotomy</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[Why are there so many programming languages? One of the driving reasons for this is that some
languages tend to produce fast code, but are a bit of a pain to use (C++), while others are a breeze
to write, but run somewhat slow (Python). Depending on the ratio of CPUs to programmers, one or the
other might be relatively more important.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/10/06/ousterhouts-dichotomy.html"><![CDATA[
<h1><span>On Ousterhout</span>&rsquo;<span>s Dichotomy</span> <time class="meta" datetime="2024-10-06">Oct 6, 2024</time></h1>
<p><span>Why are there so many programming languages? One of the driving reasons for this is that some</span>
<span>languages tend to produce fast code, but are a bit of a pain to use (C++), while others are a breeze</span>
<span>to write, but run somewhat slow (Python). Depending on the ratio of CPUs to programmers, one or the</span>
<span>other might be relatively more important.</span></p>
<p><span>But can</span>&rsquo;<span>t we just, like, implement a universal language that is convenient but slowish by default,</span>
<span>but allows an expert programmer to drop to a lower, more performant but harder register? I think</span>
<span>there were many attempts at this, and they didn</span>&rsquo;<span>t quite work out.</span></p>
<p><span>The </span><em><span>natural</span></em><span> way to go about this is to start from the high-level side. Build a high-level</span>
<span>featureful language with large runtime, and then provide granular opt outs of specific runtime</span>
<span>facilities. Two great examples here are C# and D. And the most famous example of this paradigm is</span>
<span>Python, with </span>&ldquo;<span>rewrite slow parts in C</span>&rdquo;<span> mantra.</span></p>
<p><span>It seems to me that such an approach can indeed solve the </span>&ldquo;<span>easy to use</span>&rdquo;<span> part of the dichotomy, but</span>
<span>doesn</span>&rsquo;<span>t quite work as promised for </span>&ldquo;<span>runs fast</span>&rdquo;<span> one. And here</span>&rsquo;<span>s the reason. For performance, what</span>
<span>matters is not so much the code that</span>&rsquo;<span>s executed, but rather the layout of objects in memory. And the</span>
<span>high-level dialect locks-in pointer-heavy GC object model! Even if you write your code in assembly,</span>
<span>the performance ceiling will be determined by all those pointers GC needs. To actually get full</span>
&ldquo;<span>low-level</span>&rdquo;<span> performance, you need to effectively </span>&ldquo;<span>mirror</span>&rdquo;<span> the data across the dialects across a</span>
<span>quasi-FFI boundary.</span></p>
<p><span>And that</span>&rsquo;<span>s what kills </span>&ldquo;<span>write most of the code in Python, rewrite hot spots in C</span>&rdquo;<span> approach </span>&mdash;<span> the</span>
<span>overhead for transitioning between the native C data structures and the Python ones tends to eat any</span>
<span>performance benefits that C brings to the table. There are some very real, very important</span>
<span>exceptions, where it is possible to batch sufficiently large packages of work to minimize the</span>
<span>overhead: </span><span class="display"><a href="http://venge.net/graydon/talks/VectorizedInterpretersTalk-2023-05-12.pdf" class="url">http://venge.net/graydon/talks/VectorizedInterpretersTalk-2023-05-12.pdf</a><span>.</span></span>
<span>But it seems that the average case looks more like this:</span>
<span class="display"><a href="https://code.visualstudio.com/blogs/2018/03/23/text-buffer-reimplementation" class="url">https://code.visualstudio.com/blogs/2018/03/23/text-buffer-reimplementation</a><span>.</span></span></p>
<p><span>And this brings me to Rust. It feels like it </span><em><span>accidentally</span></em><span> blundered into the space of universal</span>
<span>languages through the floor. There are no heavy runtime-features to opt out of in Rust. The object</span>
<span>model is universal throughout the language. There isn</span>&rsquo;<span>t a value-semantics/reference-semantics</span>
<span>dichotomy, references are first-class values. And yet:</span></p>
<ul>
<li>
<span>There</span>&rsquo;<span>s memory safety, which removes most of the fun aspects of low-level programming.</span>
</li>
<li>
<span>The language didn</span>&rsquo;<span>t sleep on basic PL niceties like sum-types, generics and</span>
&ldquo;<span>everything-is-expression</span>&rdquo;<span>.</span>
</li>
<li>
<span>And a healthy minority of rubyists in the community worked tirelessly to ensure that </span><a href="https://robert.ocallahan.org/2016/08/random-thoughts-on-rust-cratesio-and.html"><span>systems</span>
<span>programmers can have nice</span>
<span>things</span></a><span>.</span>
</li>
</ul>
<p><span>As a result, there is a certain spectrum of Rust:</span></p>
<ul>
<li>
<span>Sloppy Rust, which allocates and clones left-and-right.</span>
</li>
<li>
<span>Normal Rust, which opportunistically uses pretzels and avoids gratuitous allocations but otherwise</span>
<span>doesn</span>&rsquo;<span>t try to optimize anything specifically.</span>
</li>
<li>
<span>DoD Rust, which thinks a bit about cache-lines, packs things into arenas, uses indexes instead of</span>
<span>pointers with an occasional SoA and SIMD.</span>
</li>
<li>
<span>Crazy here-be-dragons Rust with untagged unions, unsafe, inline assembly and other wizardry.</span>
</li>
</ul>
<p><span>While the bottom end here sits pretty comfortably next to C, the upper tip doesn</span>&rsquo;<span>t quite reach the</span>
<span>usability level of Python. But this is mostly compensated through these three effects:</span></p>
<ul>
<li>
<span>Unified object model ensures that there</span>&rsquo;<span>s no performance tax and little ceremony when going up and,</span>
<span>down performance sloppiness spectrum.</span>
</li>
<li>
<a href="https://smallcultfollowing.com/babysteps/blog/2016/05/23/unsafe-abstractions/"><span>Unsafe abstractions</span></a>
<span>not only allow an expert programmer to write optimal code, but, crucially, they allow wrapping it</span>
<span>into misuse-resistance interface, which a non-expert programmer can easily use from a high-level</span>
<span>Rust dialect.</span>
</li>
<li>
<span>Performance </span><em><span>option</span></em><span> is quite an unfair advantage. When you start writing something, you don</span>&rsquo;<span>t</span>
<span>necessary know how fast the thing would have to be. It often depends on the uncertain future. But,</span>
<span>if you can sacrifice just a tiny bit of developer experience to get an insurance that, if push</span>
<span>comes to shove, you could incrementally arrive at the optimal performance without whole-system</span>
<span>rewrites, that is often a hard-to-refuse offer.</span>
</li>
</ul>
]]></content>
</entry>

<entry>
<title type="text">The Watermelon Operator</title>
<link href="https://matklad.github.io/2024/09/24/watermelon-operator.html" rel="alternate" type="text/html" title="The Watermelon Operator" />
<published>2024-09-24T00:00:00+00:00</published>
<updated>2024-09-24T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/09/24/watermelon-operator</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[In these two most excellent articles,
https://without.boats/blog/let-futures-be-futures
and
https://without.boats/blog/futures-unordered, 
withoutboats introduces the concepts of multi-task and intra-task concurrency.
I want to revisit this distinction --- while I agree that there are different classes
of patterns of concurrency here, I am not quite satisfied with this specific partitioning of the
design space. I will use Rust-like syntax for most of the examples, but I am more interested in the
language-agnostic patterns, rather than in Rust's specific implementation of async.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/09/24/watermelon-operator.html"><![CDATA[
<h1><span>The Watermelon Operator</span> <time class="meta" datetime="2024-09-24">Sep 24, 2024</time></h1>
<p><span>In these two most excellent articles,</span>
<a href="https://without.boats/blog/let-futures-be-futures" class="display url">https://without.boats/blog/let-futures-be-futures</a>
<span>and</span>
<span class="display"><a href="https://without.boats/blog/futures-unordered" class="url">https://without.boats/blog/futures-unordered</a><span>, </span></span>
<span>withoutboats introduces the concepts of </span>&ldquo;<span>multi-task</span>&rdquo;<span> and </span>&ldquo;<span>intra-task</span>&rdquo;<span> concurrency.</span>
<span>I want to revisit this distinction </span>&mdash;<span> while I agree that there are different classes</span>
<span>of patterns of concurrency here, I am not quite satisfied with this specific partitioning of the</span>
<span>design space. I will use Rust-like syntax for most of the examples, but I am more interested in the</span>
<span>language-agnostic patterns, rather than in Rust</span>&rsquo;<span>s specific implementation of async.</span></p>
<section id="The-Two-Examples">

    <h2>
    <a href="#The-Two-Examples"><span>The Two Examples</span> </a>
    </h2>
<p><span>Let</span>&rsquo;<span>s introduce the two kinds of concurrency using a somewhat abstract example. We want to handle a</span>
<code>Request</code><span> by doing some computation and then persisting the results in the database and in the cache.</span>
<span>Notably, writes to the cache and to the database can proceed concurrently. So, something like this:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">process</span>(</span>
<span class="line">  db: Database,</span>
<span class="line">  cache: Cache,</span>
<span class="line">  request: Request,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> Response {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">response</span> = <span class="hl-title function_ invoke__">compute_response</span>(db, cache, request).<span class="hl-keyword">await</span>;</span>
<span class="line">  <span class="hl-title function_ invoke__">spawn</span>(<span class="hl-title function_ invoke__">update_db</span>(db, response));</span>
<span class="line">  <span class="hl-title function_ invoke__">spawn</span>(<span class="hl-title function_ invoke__">update_cache</span>(cache, response));</span>
<span class="line">  response</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">update_db</span>(db: Database, response: Response);</span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">update_cache</span>(cache: Cache, response: Response);</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">spawn</span>&lt;T&gt;(f: <span class="hl-keyword">impl</span> <span class="hl-title class_">Future</span>&lt;Output = T&gt;) <span class="hl-punctuation">-&gt;</span> JoinHandle&lt;T&gt;;</span></code></pre>

</figure>
<p><span>This is multi-task concurrency style </span>&mdash;<span> we fire off two tasks for updating the database and the</span>
<span>cache. Here</span>&rsquo;<span>s the same snippet in intra-task style, where we use join function on futures:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">process</span>(</span>
<span class="line">  db: Database,</span>
<span class="line">  cache: Cache,</span>
<span class="line">  request: Request,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> Response {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">response</span> = <span class="hl-title function_ invoke__">compute_response</span>(db, cache, request).<span class="hl-keyword">await</span>;</span>
<span class="line">  <span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">    <span class="hl-title function_ invoke__">update_db</span>(db, response),</span>
<span class="line">    <span class="hl-title function_ invoke__">update_cache</span>(cache, response),</span>
<span class="line">  ).<span class="hl-keyword">await</span>;</span>
<span class="line">  response</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">update_db</span>(db: Database, response: Response) { ... }</span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">update_cache</span>(cache: Cache, response: Response) { ... }</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">join</span>&lt;U, V&gt;(</span>
<span class="line">  f: <span class="hl-keyword">impl</span> <span class="hl-title class_">Future</span>&lt;Output = U&gt;,</span>
<span class="line">  g: <span class="hl-keyword">impl</span> <span class="hl-title class_">Future</span>&lt;Output = V&gt;,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> (U, V);</span></code></pre>

</figure>
<p><span>In other words:</span></p>
<p><span>Multi-task concurrency uses </span><code>spawn</code><span> </span>&mdash;<span> an operation that takes a future and starts a tasks that</span>
<span>executes independently of the parent task.</span></p>
<p><span>Intra-task concurrency uses </span><code>join</code><span> </span>&mdash;<span> an operation that takes a pair of futures and executes them</span>
<span>concurrently as a part of the current task.</span></p>
<p><span>But what is the </span><em><span>actual</span></em><span> difference between the two?</span></p>
</section>
<section id="Parallelism-is-not">

    <h2>
    <a href="#Parallelism-is-not"><span>Parallelism is not</span> </a>
    </h2>
<p><span>One candidate is parallelism </span>&mdash;<span> with </span><code>spawn</code><span>, the tasks can run not only concurrently, but actually</span>
<span>in parallel, on different CPU cores. </span><code>join</code><span> restricts them to the same thread that runs the main</span>
<span>task. But I think this is not quite right, abstractly, and is more of a product of specific Rust</span>
<span>APIs. There </span><em><span>are</span></em><span> executors which spawn on the current thread only. And, while in Rust it</span>&rsquo;<span>s not</span>
<span>really possible to make </span><code>join</code><span> poll the futures in parallel, I </span><em><span>think</span></em><span> this is just an artifact of</span>
<span>Rust existing API design (futures can</span>&rsquo;<span>t opt-out of synchronous cancellation). In other words, I</span>
<span>think it is possible in theory to implement an async runtime which provides all of the following</span>
<span>functions at the same time:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">spawn</span>&lt;F&gt;(fut: F) <span class="hl-punctuation">-&gt;</span> JoinHandle&lt;Output = F::Output&gt;</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F: Future;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">pspawn</span>&lt;F&gt;(fut: F) <span class="hl-punctuation">-&gt;</span> PJoinHandle&lt;Output = F&gt;</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F: Future + <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;static</span>,</span>
<span class="line">  F::Output: <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;static</span>;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">join</span>&lt;F1, F2&gt;(</span>
<span class="line">  fut1: F1,</span>
<span class="line">  fut2: F2,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> (F1::Output, F2::Output)</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F1: Future,</span>
<span class="line">  F2: Future;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">pjoin</span>&lt;F1, F2&gt;(</span>
<span class="line">  fut1: F1,</span>
<span class="line">  fut2: F2,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> (F1::Output, F2::Output)</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F1: Future + <span class="hl-built_in">Send</span>, <span class="hl-comment">// NB: only Send, no &#x27;static</span></span>
<span class="line">  F1::Output:  <span class="hl-built_in">Send</span>,</span>
<span class="line">  F2: Future + <span class="hl-built_in">Send</span>,</span>
<span class="line">  F2::Output:  <span class="hl-built_in">Send</span>;</span></code></pre>

</figure>
<p><span>To confuse matters further, let</span>&rsquo;<span>s rewrite our example in TypeScript:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">function</span> <span class="hl-title function_">process</span>(<span class="hl-params"></span></span>
<span class="line"><span class="hl-params">  db: Database,</span></span>
<span class="line"><span class="hl-params">  cache: Cache,</span></span>
<span class="line"><span class="hl-params">  request: Request,</span></span>
<span class="line"><span class="hl-params"></span>): <span class="hl-title class_">Response</span> {</span>
<span class="line">  <span class="hl-keyword">const</span> response = <span class="hl-keyword">await</span> <span class="hl-title function_">compute_response</span>(db, cache, request);</span>
<span class="line">  <span class="hl-keyword">const</span> db_update = <span class="hl-title function_">update_db</span>(db, response);</span>
<span class="line">  <span class="hl-keyword">const</span> cache_update = <span class="hl-title function_">update_cache</span>(cache, response);</span>
<span class="line">  <span class="hl-keyword">await</span> <span class="hl-title class_">Promise</span>.<span class="hl-title function_">all</span>([db_update, cache_update]);</span>
<span class="line">  <span class="hl-keyword">return</span> response</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>and using Rust</span>&rsquo;<span>s rayon library:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">process</span>(</span>
<span class="line">  db: Database,</span>
<span class="line">  cache: Cache,</span>
<span class="line">  request: Request,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> Response {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">response</span> = <span class="hl-title function_ invoke__">compute_response</span>(db, cache, request).<span class="hl-keyword">await</span>;</span>
<span class="line">  rayon::<span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">    || <span class="hl-title function_ invoke__">update_db</span>(db, response),</span>
<span class="line">    || <span class="hl-title function_ invoke__">update_cache</span>(cache, response),</span>
<span class="line">  );</span>
<span class="line">  response</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Are these examples multi-task or intra-task? To me, the TypeScript one feels multi-task </span>&mdash;<span> although</span>
<span>it is syntactically close to </span><code>join().async</code><span>, the two update promises are running independently from</span>
<span>the parent task. If we forget the call to </span><code>Promise.all</code><span>, the cache and the database would still get</span>
<span>updated (but likely </span><em><span>after</span></em><span> we would have returned the response to the user)! In contrast, rayon</span>
<span>feels intra-task </span>&mdash;<span> although the closures could get stolen and be run by a different thread, they</span>
<span>won</span>&rsquo;<span>t </span>&ldquo;<span>escape</span>&rdquo;<span> dynamic extent of the encompassing </span><code>process</code><span> call.</span></p>
</section>
<section id="To-await-or-await-to">

    <h2>
    <a href="#To-await-or-await-to"><span>To await or await to?</span> </a>
    </h2>
<p><span>Let</span>&rsquo;<span>s zoom in onto the JS and the join examples:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">function</span> <span class="hl-title function_">process</span>(<span class="hl-params"></span></span>
<span class="line"><span class="hl-params">  db: Database,</span></span>
<span class="line"><span class="hl-params">  cache: Cache,</span></span>
<span class="line"><span class="hl-params">  request: Request</span></span>
<span class="line"><span class="hl-params"></span>): <span class="hl-title class_">Response</span> {</span>
<span class="line">  <span class="hl-keyword">const</span> response = <span class="hl-keyword">await</span> <span class="hl-title function_">compute_response</span>(db, cache, request);</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">await</span> <span class="hl-title class_">Promise</span>.<span class="hl-title function_">all</span>([</span>
<span class="line">    <span class="hl-title function_">update_db</span>(db, response),</span>
<span class="line">    <span class="hl-title function_">update_cache</span>(cache, response),</span>
<span class="line">  ]);</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">return</span> response;</span>
<span class="line">}</span></code></pre>

</figure>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">process</span>(</span>
<span class="line">  db: Database,</span>
<span class="line">  cache: Cache,</span>
<span class="line">  request: Request,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> Response {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">response</span> = <span class="hl-title function_ invoke__">compute_response</span>(db, cache, request).<span class="hl-keyword">await</span>;</span>
<span class="line"></span>
<span class="line">  <span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">    <span class="hl-title function_ invoke__">update_db</span>(db, response),</span>
<span class="line">    <span class="hl-title function_ invoke__">update_cache</span>(cache, response),</span>
<span class="line">  ).<span class="hl-keyword">await</span>;</span>
<span class="line"></span>
<span class="line">  response</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>I</span>&rsquo;<span>ve re-written the JavaScript version to be syntactically isomorphic to the Rust one. The</span>
<span>difference is on the semantic level: JavaScript promises are eager, they start executing as soon as</span>
<span>a promise is created. In contrast, Rust futures are lazy </span>&mdash;<span> they do nothing until polled. And</span>
<em><span>this</span></em><span> I think is the fundamental difference, it is lazy vs. eager </span>&ldquo;<span>futures</span>&rdquo;<span> (</span><code>thread::spawn</code><span> is an</span>
<span>eager </span>&ldquo;<span>future</span>&rdquo;<span> while </span><code>rayon::join</code><span> a lazy one).</span></p>
<p><span>And it seems that lazy semantics is quite a bit more elegant! The beauty of</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">  <span class="hl-title function_ invoke__">update_db</span>(db, response),</span>
<span class="line">  <span class="hl-title function_ invoke__">update_cache</span>(cache, response),</span>
<span class="line">).<span class="hl-keyword">await</span>;</span></code></pre>

</figure>
<p><span>is that it</span>&rsquo;<span>s Molière</span>&rsquo;<span>s prose </span>&mdash;<span> this is structured concurrency, but without bundles, nurseries,</span>
<span>scopes, and other weird APIs.</span></p>
<p><span>It makes runtime semantics nicer even in dynamically typed languages. In JavaScript, forgetting an</span>
<span>await is a common, and very hard to spot problem </span>&mdash;<span> without await, code still works, but is</span>
<em><span>sometimes</span></em><span> wrong (if the async operation doesn</span>&rsquo;<span>t finish quite as fast as usual). Imagine JS with</span>
<span>lazy promises </span>&mdash;<span> there, forgetting an </span><code>await</code><span> would </span><em><span>always</span></em><span> consistently break. So, the need to</span>
<span>statically lint missing awaits will be less pressing.</span></p>
<p><span>Compare this with Erlang</span>&rsquo;<span>s take on nulls: while in typical dynamically typed languages partial</span>
<span>functions can return a value </span><code>T</code><span> or a </span><code>None</code><span>, in Erlang the convention is to return either </span><code>{ok, T}</code>
<span>or </span><code>none</code><span>. That is, even if the value is non-null, the call-site is </span><em><span>forced</span></em><span> to unpack it, you can</span>&rsquo;<span>t</span>
<span>write code that happens to work as long as </span><code>T</code><span> is non-null.</span></p>
<p><span>And of course, in Rust, the killer feature of lazy futures is that you can just borrow data from the</span>
<span>enclosing scope.</span></p>
<p><span>But it seems like there is one difference between multi-task and intra-task concurrency.</span></p>
</section>
<section id="One-Two-N-and-More">

    <h2>
    <a href="#One-Two-N-and-More"><span>One, Two, N, and More</span> </a>
    </h2>
<p><span>In the words of withoutboats:</span></p>

<figure class="blockquote">
<blockquote><p><span>The first limitation is that it is only possible to achieve a static arity of concurrency with</span>
<span>intra-task concurrency. That is, you cannot join (or select, etc) an arbitrary number of futures</span>
<span>with intra-task concurrency: the number must be fixed at compile time.</span></p>
</blockquote>

</figure>
<p><span>That is, you can do</span>
<span class="display"><code>join(a, b).await</code><span>,</span></span>
<span>and</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">  <span class="hl-title function_ invoke__">join</span>(a, b)</span>
<span class="line">  c,</span>
<span class="line">).<span class="hl-keyword">await</span></span></code></pre>

</figure>
<p><span>and, with some macros, even</span></p>

<figure class="code-block">


<pre><code><span class="line">join!(a, b, c, d, e, f).<span class="hl-keyword">await</span>;</span></code></pre>

</figure>
<p><span>but you can</span>&rsquo;<span>t do </span><span class="display"><code>join(xs...).await</code><span>.</span></span></p>
<p><span>I think this is incorrect, in a trivial and in an interesting way.</span></p>
<p><span>The trivial incorrectness is that there</span>&rsquo;<span>s </span><code>join_all</code><span>, that takes a slice of futures and is a direct</span>
<span>generalization of </span><code>join</code><span> to a runtime-variable number of futures.</span></p>
<p><span>But </span><code>join_all</code><span> still can</span>&rsquo;<span>t express the case where you don</span>&rsquo;<span>t know the number of futures up-front,</span>
<span>where you spawn some work, and only later realize that you need to spawn some more.</span></p>
<p><span>This is sort-of possible to express with </span><code>FuturesUnordered</code><span>, but that</span>&rsquo;<span>s a yuck API. I mean, even</span>
<span>its name screams </span>&ldquo;<span>DO NOT USE ME!</span>&rdquo;<span>.</span></p>
<p><span>But I do think that this is just an unfortunate API, and that the pattern actually can be expressed</span>
<span>in intra-task concurrency style nicely.</span></p>
<p><span>Let</span>&rsquo;<span>s take a closer look at the base case, </span><code>join</code><span>!</span></p>
</section>
<section id="Asynchronous-Semicolon">

    <h2>
    <a href="#Asynchronous-Semicolon"><span>Asynchronous Semicolon</span> </a>
    </h2>
<p><span>Section title is a bit of a giveaway. The </span><code>join</code><span> operator </span><em><span>is</span></em><span> </span><code>async ;</code><span>. The semicolon is an</span>
<span>operator of sequential composition:</span>
<code class="display">A; B</code></p>
<p><span>runs </span><code>A</code><span> first and then </span><code>B</code><span>.</span></p>
<p><span>In contrast, </span><code>join</code><span> is concurrent composition:</span>
<code class="display">join(A, B)</code></p>
<p><span>runs </span><code>A</code><span> and </span><code>B</code><span> concurrently.</span></p>
<p><span>And both </span><code>join</code><span> and </span><code>;</code><span> share the same problem </span>&mdash;<span> they can compose only a finite number of things.</span></p>
<p><span>But that</span>&rsquo;<span>s why we have other operators for sequential composition! If we know how many things we</span>
<span>need to run, we can use a counted </span><code>for</code><span> loop. And </span><code>join_all</code><span> is an analogue of a counted for loop!</span></p>
<p><span>In case where we </span><em><span>don</span>&rsquo;<span>t</span></em><span> know up-front when to stop, we use a </span><code>while</code><span>. And this is exactly what we</span>
<span>miss </span>&mdash;<span> there</span>&rsquo;<span>s no concurrently-flavored </span><code>while</code><span> operator.</span></p>
<p><span>Importantly, what we are looking for is </span><em><span>not</span></em><span> an async for:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">for</span> <span class="hl-variable">x</span> <span class="hl-keyword">in</span> iter {</span>
<span class="line">  <span class="hl-title function_ invoke__">process</span>(x).<span class="hl-keyword">await</span>;</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Here, although there could be some concurrency inside a single loop iteration, the iterations</span>
<span>themselves are run sequentially. The second iteration starts only when the first one finished.</span>
<span>Pictorially, this looks like a spiral, or a loop if we look from the side:</span></p>

<figure>

<img alt="" src="https://github.com/user-attachments/assets/92a9c1dd-1f94-4fac-9e11-5f0c54e6e10e">
</figure>
<p><span>What we rather want is to run </span><em><span>many</span></em><span> copies of the body concurrently, something like this:</span></p>

<figure>

<img alt="" src="https://github.com/user-attachments/assets/7149b6e8-2d99-4837-911b-36fbee80134a">
</figure>
<p><span>A spindle-like shape with many concurrent strands, which looks like wheel</span>&rsquo;<span>s spokes from the side.</span>
<span>Or, if you are </span><em><span>really</span></em><span> short on fitting metaphors:</span></p>
</section>
<section id="The-Watermelon-Operator-1">

    <h2>
    <a href="#The-Watermelon-Operator-1"><span>The Watermelon Operator</span> </a>
    </h2>
<p><span>Now, I understand that I</span>&rsquo;<span>ve already poked fun at unfortunate </span><code>FuturesUnordered</code><span> name, but I can</span>&rsquo;<span>t</span>
<span>really find a fitting name for the construct we want here. So I am going to boringly use</span>
<code>concurrently</code><span> keyword, which is way too long, but I</span>&rsquo;<span>ll refer to it as </span>&ldquo;<span>the watermelon operator</span>&rdquo;
<span>The stripes on the watermelon resemble the independent strands of execution this operator creates:</span></p>

<figure>

<img alt="wikipedia watermelons" src="https://github.com/user-attachments/assets/f0760a4a-03ba-45a5-bc26-e9156e28c5c9">
</figure>
<p><span>So, if you are writing a TCP server, your accept loop could look like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">concurrently <span class="hl-keyword">let</span> <span class="hl-variable">Some</span>(socket) = listener.<span class="hl-title function_ invoke__">accept</span>().<span class="hl-keyword">await</span> <span class="hl-keyword">in</span> {</span>
<span class="line">  <span class="hl-title function_ invoke__">handle_connection</span>(socket).<span class="hl-keyword">await</span>;</span>
<span class="line">}.<span class="hl-keyword">await</span></span></code></pre>

</figure>
<p><span>This runs accept in a loop, and, for each accepted socket, runs </span><code>handle_connection</code><span> concurrently.</span>
<span>There are as many concurrent </span><code>handle_connection</code><span> calls as there are ready sockets in our listener!</span></p>
<p><span>Let</span>&rsquo;<span>s limit the maximum number of concurrent connections, to provide back pressure:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">let</span> <span class="hl-variable">semaphore</span> = Semaphore::<span class="hl-title function_ invoke__">new</span>(<span class="hl-number">16</span>);</span>
<span class="line"></span>
<span class="line">concurrently</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">Some</span>((socket, permit)) = <span class="hl-keyword">try</span> {</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">permit</span> = semaphore.<span class="hl-title function_ invoke__">acquire</span>().<span class="hl-keyword">await</span>;</span>
<span class="line">    <span class="hl-keyword">let</span> <span class="hl-variable">socket</span> = listener.<span class="hl-title function_ invoke__">accept</span>().<span class="hl-keyword">await</span>?;</span>
<span class="line">    (socket, permit)</span>
<span class="line">  }</span>
<span class="line"><span class="hl-keyword">in</span> {</span>
<span class="line">  <span class="hl-title function_ invoke__">handle_connection</span>(socket).<span class="hl-keyword">await</span>;</span>
<span class="line">  <span class="hl-title function_ invoke__">drop</span>(permit);</span>
<span class="line">}.<span class="hl-keyword">await</span></span></code></pre>

</figure>
<p><span>You get the idea (hopefully):</span></p>
<ul>
<li>
<span>In the </span>&ldquo;<span>head</span>&rdquo;<span> of our concurrent loop (cooloop?) construct, we first acquire a semaphore permit</span>
<span>and then fetch a socket.</span>
</li>
<li>
<span>Both the socket and the permit are passed to the body.</span>
</li>
<li>
<span>The body releases the permit at the end.</span>
</li>
<li>
<span>While the </span>&ldquo;<span>head</span>&rdquo;<span> construct runs in a loop concurrently to bodies, it is throttled by the minimum</span>
<span>of the available permits and ready connections.</span>
</li>
</ul>
<p><span>To make this more concrete, let</span>&rsquo;<span>s spell this out as a library</span>
<span>function:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">join</span>&lt;F1, F2&gt;(</span>
<span class="line">  fut1: F1,</span>
<span class="line">  fut2: F2,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> (F1::Output, F2::Output)</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F1: Future,</span>
<span class="line">  F2: Future;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">join_all</span>&lt;F&gt;(futs: <span class="hl-type">Vec</span>&lt;F&gt;) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">Vec</span>&lt;F::Output&gt;</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F: Future;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">concurrently</span>&lt;C, FC, B, FB, T&gt;(condition: C, body: B)</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  C: <span class="hl-title function_ invoke__">FnMut</span>() <span class="hl-punctuation">-&gt;</span> FC,</span>
<span class="line">  FC: Future&lt;Output = <span class="hl-type">Option</span>&lt;T&gt;&gt;,</span>
<span class="line">  B: <span class="hl-title function_ invoke__">FnMut</span>(T) <span class="hl-punctuation">-&gt;</span> FB,</span>
<span class="line">  FB: Future&lt;Output = ()&gt;;</span></code></pre>

</figure>
<p><span>I claim that this is the full set of </span>&ldquo;<span>primitive</span>&rdquo;<span> operations needed to express</span>
<span>more-or-less everything in intra-task concurrency style.</span></p>
<p><span>In particular, we can implement multi-task concurrency this way! To do so, we</span>&rsquo;<span>ll write a universal</span>
<span>watermelon operator, where the </span><code>T</code><span> which is passed to the body is an</span>
<span class="display"><code>Box&lt;dyn Future&lt;Output=()&gt;&gt;</code><span>,</span></span>
<span>and where the body just runs this future:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">multi_task_concurrency_main</span>(</span>
<span class="line">  spawn: <span class="hl-keyword">impl</span> <span class="hl-title class_">Fn</span>(<span class="hl-keyword">impl</span> <span class="hl-title class_">Future</span>&lt;Output = ()&gt; + <span class="hl-symbol">&#x27;static</span>),</span>
<span class="line">) {</span>
<span class="line">    ...</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">type</span> <span class="hl-title class_">AnyFuture</span> = <span class="hl-type">Box</span>&lt;<span class="hl-keyword">dyn</span> Future&lt;Output = ()&gt; + <span class="hl-symbol">&#x27;static</span>&gt;;</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">universal_watermelon</span>() {</span>
<span class="line">  <span class="hl-keyword">let</span> (sender, receiver) = channel::&lt;AnyFuture&gt;();</span>
<span class="line">  <span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">    <span class="hl-title function_ invoke__">multi_task_concurrency_main</span>(<span class="hl-keyword">move</span> |fut| {</span>
<span class="line">      sender.<span class="hl-title function_ invoke__">send</span>(<span class="hl-type">Box</span>::<span class="hl-title function_ invoke__">new</span>(fut))</span>
<span class="line">    }),</span>
<span class="line">    <span class="hl-title function_ invoke__">concurrently</span>(</span>
<span class="line">      || <span class="hl-keyword">async</span> {</span>
<span class="line">        receiver.<span class="hl-title function_ invoke__">recv</span>().<span class="hl-keyword">await</span>;</span>
<span class="line">      },</span>
<span class="line">      |fut| <span class="hl-keyword">async</span> {</span>
<span class="line">        fut.<span class="hl-keyword">await</span>;</span>
<span class="line">      },</span>
<span class="line">    ),</span>
<span class="line">  )</span>
<span class="line">  .<span class="hl-keyword">await</span>;</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Note that the conversion in the opposite direction is not possible! With intra-task concurrency, we</span>
<span>can borrow from the parent stack frame. So it is not a problem to </span><em><span>restrict</span></em><span> that to only allow</span>
<code>'static</code><span> futures into the channel. In a sense, in the above example we return the future up the</span>
<span>stack, which explains why it can</span>&rsquo;<span>t borrow locals from our stack frame.</span></p>
<p><span>With multi-task concurrency though, we </span><em><span>start</span></em><span> with static futures. To let them borrow any stack data</span>
<span>requires unsafe.</span></p>
<p><span>Note also that the above set of operators, </span><code>join</code><span>, </span><code>join_all</code><span>, </span><code>concurrently</code><span> is </span><em><span>orthogonal</span></em><span> to</span>
<span>parallelism. Alongside those operators, there could exist </span><code>pjoin</code><span>, </span><code>pjoin_all</code><span> and </span><code>pconcurrently</code>
<span>with the </span><code>Send</code><span> bounds, such that you could mix and match parallel and single-core concurrency.</span></p>
</section>
<section id="If-a-Stack-is-a-Tree-Does-it-Make-Any-Difference">

    <h2>
    <a href="#If-a-Stack-is-a-Tree-Does-it-Make-Any-Difference"><span>If a Stack is a Tree, Does it Make Any Difference?</span> </a>
    </h2>
<p><span>One possible objection to the above framing of watermelon as a language-level operator is that it</span>
<span>seemingly doesn</span>&rsquo;<span>t pass zero-cost abstraction test. It </span><em><span>can</span></em><span> start an unbounded number of futures,</span>
<span>and those futures have to be stored </span><em><span>somewhere</span></em><span>. So we have a language operator which requires</span>
<span>dynamic memory allocation, which is a big no-no for any systems programming language.</span></p>
<p><span>I think there </span><em><span>is</span></em><span> some truth to it, and not an insignificant amount of it, but I think I can maybe</span>
<span>weasel out of it.</span></p>
<p><span>Consider recursion. Recursion </span><em><span>also</span></em><span> can allocate arbitrary amount of memory (on the stack), but</span>
<span>that is considered fine (I would </span><em><span>also</span></em><span> agree that it is not in fact fine that unbounded recursion</span>
<span>is considered fine, but, for the scope of this discussion, I will be a hypocrite and will ignore</span>
<span>that opinion of mine).</span></p>
<p><span>And here, we have essentially the same situation </span>&mdash;<span> we want to allocate arbitrary many (async)</span>
<span>stack frames, arranged in a tree. Doing it </span>&ldquo;<span>on the heap</span>&rdquo;<span> is easy, but we don</span>&rsquo;<span>t like the heap here.</span>
<span>Luckily, I believe there</span>&rsquo;<span>s a compilation scheme (hat tip to </span><a href="https://www.abubalay.com"><span>@rpjohnst</span></a>
<span>for patiently explaining it to me five times in different words) that implements this more-or-less</span>
<span>as efficiently as the normal call stack.</span></p>
<p><span>The idea is that we will have </span><em><span>two</span></em><span> stacks </span>&mdash;<span> a sync one and an async one. Specifically:</span></p>
<ul>
<li>
<span>Every sync function we compile normally, with a single stack.</span>
</li>
<li>
<span>Async functions get </span><em><span>two</span></em><span> stack pointers. So, we burn </span><code>sp</code><span> and one other register</span>
<span>(let</span>&rsquo;<span>s call it </span><code>asp</code><span>).</span>
</li>
<li>
<span>If an async function calls a sync function, the callee</span>&rsquo;<span>s frame is pushed onto </span><code>sp</code><span>.</span>
<span>Crucially, because sync functions can only call other </span><code>sync</code><span> functions, the callee doesn</span>&rsquo;<span>t need</span>
<span>to know the value of </span><code>asp</code><span>.</span>
</li>
<li>
<span>If an async function calls another async function, the frame (specifically, the </span>&ldquo;<span>variables live</span>
<span>across await point</span>&rdquo;<span> part of it) is pushed onto </span><code>asp</code><span>.</span>
</li>
<li>
<span>This async stack is segmented. So, for async function calls, we also do a check for </span>&ldquo;<span>do we have</span>
<span>enough stack?</span>&rdquo;<span> and, if not, allocate a new segment, linking them via a frame pointer.</span>
</li>
<li>
&ldquo;<span>Allocating a new segment</span>&rdquo;<span> doesn</span>&rsquo;<span>t mean that we actually go and call malloc. Rather, there</span>&rsquo;<span>s a</span>
<span>fixed-sized contiguous slab of say, 8 megs, out of which all async frames are allocated.</span>
</li>
<li>
<span>If we are out of async-stack, we crash in pretty much the same way as for the boring sync stack</span>
<span>overflow.</span>
</li>
</ul>
<p><span>While this </span><em><span>looks</span></em><span> just like Go-style segmented stacks, I think this scheme is quite a bit more</span>
<span>efficient (warning: I in general have a tendency to confidently talk about things I know little</span>
<span>about, and this one is the extreme case of that. If some Go compiler engineer disagrees with me, I</span>
<span>am probably in the wrong!).</span></p>
<p><span>The main difference is that the distinction between sync and async functions is maintained in the</span>
<span>type system. There are </span><em><span>no</span></em><span> changes for sync functions at all, so the principle of don</span>&rsquo;<span>t pay for</span>
<span>what you don</span>&rsquo;<span>t use is observed. This is in contrast to Go </span>&mdash;<span> I believe that Go, in general, can</span>&rsquo;<span>t</span>
<span>know whether a particular function can yield (that  is, if any function it (indirectly) calls can</span>
<span>yield), so it has to conservatively insert stack checks everywhere.</span></p>
<p><span>Then, even the async stack frames don</span>&rsquo;<span>t have to store </span><em><span>everything</span></em><span>, but just the stuff live across</span>
<span>await. Everything that happens between two awaits can go to the normal stack.</span></p>
<p><span>On top of that, async functions can still do aggressive inlining. So, the async call (and the stack</span>
<span>growth check) has to happen only for dynamically dispatched async calls!</span></p>
<p><span>Furthermore, the future trait could have some kind of </span><code>size_hint</code><span> method, which returns the lower</span>
<span>and the upper bound on the size of the stack. Fully concrete futures type-erased to </span><code>dyn Future</code>
<span>would return the exact amount </span><code>(a, Some(a))</code><span>. The caller would be </span><em><span>required</span></em><span> to allocate at least</span>
<code>a</code><span> bytes of the async stack. The callee uses that contract to elide stack checks. Unknown bound,</span>
<code>(a, None)</code><span> would only be returned if type-erased concrete future </span><em><span>itself</span></em><span> calls something</span>
<span>dynamically dispatched. So only dynamically dispatched calls would have to do stack grow checks, and</span>
<span>that cost seems negligible in comparison to the cost of missing optimizations due to inability to</span>
<span>inline.</span></p>
<p><span>Altogether, it feels like this adds up to something sufficiently cheap to just call it </span>&ldquo;<span>async stack</span>
<span>allocation</span>&rdquo;<span>.</span></p>
<p><span>I guess that</span>&rsquo;<span>s all for today? Summarizing:</span></p>
<ul>
<li>
<span>Inter-task vs intra-task distinction is mostly orthogonal to the question of parallelism.</span>
</li>
<li>
<span>I claim that this is the same distinction as between </span><em><span>eager</span></em><span> and </span><em><span>lazy</span></em><span> futures.</span>
</li>
<li>
<span>In particular, there</span>&rsquo;<span>s no principled obstacles for runtime-bounded intra-task concurrency.</span>
</li>
<li>
<span>But we do miss </span><code>FuturesUnordered</code><span>, but nice. The </span><code>concurrently</code><span> operator/function feels like a</span>
<span>sufficiently low-hanging watermelon here.</span>
</li>
<li>
<span>One wrinkle is that watermelon requires dynamic allocation, but it looks like we could just</span>
<span>completely upend the compilation strategy we use for futures, implement async segmented stacks</span>
<span>which should be pretty fast, and </span><em><span>also</span></em><span> gain nice dynamically dispatched (and recursive) async</span>
<span>functions for free?</span>
</li>
</ul>
<hr>
<p><span>Haha, just kidding! Bonus content! This really should be a separate blog post, but it is</span>
<span>tangentially related, so here we go:</span></p>
</section>
<section id="Applied-Duality">

    <h2>
    <a href="#Applied-Duality"><span>Applied Duality</span> </a>
    </h2>
<p><span>So far, we</span>&rsquo;<span>ve focused on </span><code>join</code><span>, the operator that takes two futures, and </span>&ldquo;<span>runs</span>&rdquo;<span> them concurrently,</span>
<span>returning both results as a pair. But there</span>&rsquo;<span>s a second, dual operator:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">race</span>&lt;F1, F2&gt;(</span>
<span class="line">  fut1: F1,</span>
<span class="line">  fut2: F2,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> Either&lt;F1::Output, F2::Output&gt;</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F1: Future,</span>
<span class="line">  F2: Future,</span></code></pre>

</figure>
<p><span>Like </span><code>join</code><span>, </span><code>race</code><span> runs two futures concurrently. Unlike </span><code>join</code><span>,  it returns only one result </span>&mdash;
<span>that which came first. This operator is the basis for a more general </span><code>select</code><span> facility.</span></p>
<p><span>Although </span><code>race</code><span> is dual to </span><code>join</code><span>, I don</span>&rsquo;<span>t think it is as fundamental. It is possible to have two</span>
<span>dual things, where one of them is in the basis and the other is derived. For example, it is an axiom</span>
<span>of the set theory that the union of two sets, </span><code>A ∪ B</code><span>, is a set. Although the intersection of sets,</span>
<code>A ∩ B</code><span> is a dual for union, existence of intersection is </span><em><span>not</span></em><span> an axiom. Rather, the intersection</span>
<span>is defined using axiom of specification:</span></p>

<figure class="code-block">


<pre><code><span class="line">A ∩ B := {x ∈ A : x ∈ B}</span></code></pre>

</figure>
<p><span class="display"><span>Proposition 131.7.1: </span><code>race</code><span> can be defined in terms of </span><code>join</code></span></p>
<p><span>The </span><code>race</code><span> operator is trickier than it seems. Yes, it returns the result of the future that</span>
<span>finished first, but what happens with the other one? It gets cancelled. Rust implements this</span>
<span>cancellation </span>&ldquo;<span>for free</span>&rdquo;<span>, by just dropping the future, but this is restrictive. This is precisely the</span>
<span>issue that prevents </span><code>pjoin</code><span> from working.</span></p>
<p><span>I postulate that fully general cancellation is an asynchronous protocol:</span></p>
<ol>
<li>
<span>A </span><em><span>requests</span></em><span> that B is cancelled.</span>
</li>
<li>
<span>B receives this cancellation request and starts winding down.</span>
</li>
<li>
<span>A </span><em><span>waits</span></em><span> until B is cancelled.</span>
</li>
</ol>
<p><span>That is, cancellation is not </span>&ldquo;<span>I cancel thou</span>&rdquo;<span>. Rather it is </span>&ldquo;<span>I ask you to stop, and then I</span>
<span>cooperatively wait until you do so</span>&rdquo;<span>. This is very abstract, but the following three examples should</span>
<span>help make this concrete.</span></p>
<ol>
<li>
<p><span>A is some generic asynchronous task, which offloads some computation-heavy work to a CPU pool.</span>
<span>That work (B) </span><em><span>doesn</span>&rsquo;<span>t</span></em><span> have checks for cancelled flags. So, if A is canceled, it can</span>&rsquo;<span>t really</span>
<span>stop B, which means we are violating structured concurrency.</span></p>
</li>
<li>
<p><span>A is doing async IO. Specifically, A uses </span><code>io_uring</code><span> to read data from a socket. A owns a buffer,</span>
<span>and passes a pointer to it to the kernel via </span><code>io_uring</code><span> as the target buffer for a </span><code>read</code>
<span>syscall. While A is being cancelled, the kernel writes data to this buffer. If A doesn</span>&rsquo;<span>t wait</span>
<span>until the kernel is done, buffer</span>&rsquo;<span>s memory might get reused, and the kernel would corrupt some</span>
<span>unrelated data.</span></p>
</li>
</ol>
<p><span>These examples are somewhat unsatisfactory </span>&mdash;<span> A is philosophical (who needs structured</span>
<span>concurrency?), while B is esoteric (who uses </span><code>io_uring</code><span> in 2024?). But the two can be combined into</span>
<span>something rather pedestrianly bad:</span></p>
<p><span>Like in the case A, an async task submits some work to a CPU pool. But this time the work is very</span>
<span>specific </span>&mdash;<span> computing a cryptographic checksum of a message owned by A. </span><em><span>Because</span></em><span> this is</span>
<span>cryptography, this is going to be some hyper-optimized SIMD loop which definitely won</span>&rsquo;<span>t have any</span>
<span>affordance for checking some sort of a </span><code>cancelled</code><span> flag. The loop would have to run to completion,</span>
<span>or at least to a safe point. And, because the loop checksums data owned by A, we can</span>&rsquo;<span>t destroy A</span>
<span>before the loop exits, otherwise it</span>&rsquo;<span>ll be reading garbage memory!</span></p>
<p><span>And </span><em><span>this</span></em><span> example is the reason why</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">pjoin</span>&lt;F1, F2&gt;(</span>
<span class="line">  fut1: F1,</span>
<span class="line">  fut2: F2,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> (F1::Output, F2::Output)</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F1: Future + <span class="hl-built_in">Send</span>,</span>
<span class="line">  F1::Output:  <span class="hl-built_in">Send</span>,</span>
<span class="line">  F2: Future + <span class="hl-built_in">Send</span>,</span>
<span class="line">  F2::Output:  <span class="hl-built_in">Send</span>,</span></code></pre>

</figure>
<p><span>can</span>&rsquo;<span>t be a thing in Rust </span>&mdash;<span> if </span><code>fut1</code><span> runs on a thread separate from the </span><code>pjon</code><span> future, then, if</span>
<code>pjoin</code><span> ends up being cancelled, </span><code>fut1</code><span> would be pointing at garbage. You could have</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">pjoin</span>&lt;F1, F2&gt;(</span>
<span class="line">  fut1: F1,</span>
<span class="line">  fut2: F2,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> (F1::Output, F2::Output)</span>
<span class="line"><span class="hl-keyword">where</span></span>
<span class="line">  F1: Future + <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;static</span>,</span>
<span class="line">  F1::Output:  <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;static</span>,</span>
<span class="line">  F2: Future + <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;static</span>,</span>
<span class="line">  F2::Output:  <span class="hl-built_in">Send</span> + <span class="hl-symbol">&#x27;static</span>,</span></code></pre>

</figure>
<p><span>but that removes one of the major benefits of intra-task style API </span>&mdash;<span> ability to just borrow data.</span></p>
<p><span>So the fully general cancellation should be cooperative. Let</span>&rsquo;<span>s assume that it is driven by some sort</span>
<span>of cancellation token API:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">CancellationSource</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">request_cancellation</span>(&amp;<span class="hl-keyword">self</span>) { ... }</span>
<span class="line">  <span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">await_cancellation</span>(<span class="hl-keyword">self</span>) { ...  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">async</span> <span class="hl-keyword">fn</span> <span class="hl-title function_">cancel</span>(<span class="hl-keyword">self</span>) {</span>
<span class="line">    <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">request_cancellation</span>();</span>
<span class="line">    <span class="hl-keyword">self</span>.<span class="hl-title function_ invoke__">await_cancellation</span>().<span class="hl-keyword">await</span>;</span>
<span class="line">  }</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">new_token</span>(&amp;<span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> CancellationToken { ... }</span>
<span class="line">}</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">impl</span> <span class="hl-title class_">CancellationToken</span> {</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">is_cancelled</span>(&amp;<span class="hl-keyword">self</span>) <span class="hl-punctuation">-&gt;</span> <span class="hl-type">bool</span> { ... }</span>
<span class="line">  <span class="hl-keyword">fn</span> <span class="hl-title function_">on_cancelled</span>(&amp;<span class="hl-keyword">self</span>, callback: <span class="hl-keyword">impl</span> <span class="hl-title class_">FnOnce</span>()) { ... }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>Note that the question of cancellation being cooperative is </span><em><span>orthogonal</span></em><span> to the question of explicit</span>
<span>threading of cancellation tokens! They can be threaded implicitly (cooperative, implicit</span>
<span>cancellation is how Python</span>&rsquo;<span>s trio does this, though they don</span>&rsquo;<span>t really document the cooperative part</span>
<span>(the shields stuff)).</span></p>
<p><span>With this, we can write our own </span><code>race</code><span> </span>&mdash;<span> we</span>&rsquo;<span>ll create a cancellation scope and then </span><em><span>join</span></em>
<span>modified futures, each of which would cancel the other upon completion:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">fn</span> <span class="hl-title function_">race</span>&lt;U, V&gt;(</span>
<span class="line">  fut1: <span class="hl-keyword">impl</span> <span class="hl-title class_">async</span> <span class="hl-title function_ invoke__">FnOnce</span>(&amp;CancellationToken) <span class="hl-punctuation">-&gt;</span> U,</span>
<span class="line">  fut2: <span class="hl-keyword">impl</span> <span class="hl-title class_">async</span> <span class="hl-title function_ invoke__">FnOnce</span>(&amp;CancellationToken) <span class="hl-punctuation">-&gt;</span> V,</span>
<span class="line">) <span class="hl-punctuation">-&gt;</span> Either&lt;U, V&gt; {</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">source</span> = CancellationSource::<span class="hl-title function_ invoke__">new</span>();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">token</span> = source.<span class="hl-title function_ invoke__">new_token</span>();</span>
<span class="line">  <span class="hl-keyword">let</span> <span class="hl-variable">u_or_v</span> = <span class="hl-title function_ invoke__">join</span>(</span>
<span class="line">    <span class="hl-keyword">async</span> {</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">u</span> = <span class="hl-title function_ invoke__">fut1</span>(&amp;token).<span class="hl-keyword">await</span>;</span>
<span class="line">      <span class="hl-keyword">if</span> token.<span class="hl-title function_ invoke__">is_cancelled</span>() {</span>
<span class="line">        <span class="hl-keyword">return</span> <span class="hl-literal">None</span>;</span>
<span class="line">      }</span>
<span class="line">      source.<span class="hl-title function_ invoke__">cancel</span>();</span>
<span class="line">      <span class="hl-title function_ invoke__">Some</span>(u)</span>
<span class="line">    },</span>
<span class="line">    <span class="hl-keyword">async</span> {</span>
<span class="line">      <span class="hl-keyword">let</span> <span class="hl-variable">v</span> = <span class="hl-title function_ invoke__">fut2</span>(&amp;token).<span class="hl-keyword">await</span>;</span>
<span class="line">      <span class="hl-keyword">if</span> token.<span class="hl-title function_ invoke__">is_cancelled</span>() {</span>
<span class="line">        <span class="hl-keyword">return</span> <span class="hl-literal">None</span>;</span>
<span class="line">      }</span>
<span class="line">      source.<span class="hl-title function_ invoke__">cancel</span>();</span>
<span class="line">      <span class="hl-title function_ invoke__">Some</span>(v)</span>
<span class="line">    },</span>
<span class="line">  )</span>
<span class="line">  .<span class="hl-keyword">await</span>;</span>
<span class="line">  <span class="hl-keyword">match</span> u_or_v {</span>
<span class="line">    (<span class="hl-title function_ invoke__">Some</span>(u), <span class="hl-literal">None</span>) =&gt; <span class="hl-title function_ invoke__">Left</span>(u),</span>
<span class="line">    (<span class="hl-literal">None</span>, <span class="hl-title function_ invoke__">Some</span>(v)) =&gt; <span class="hl-title function_ invoke__">Right</span>(v),</span>
<span class="line">    _ =&gt; <span class="hl-built_in">unreachable!</span>(),</span>
<span class="line">  }</span>
<span class="line">}</span></code></pre>

</figure>
<p><span>In other words, </span><code>race</code><span> is but a cooperatively-cancelled </span><code>join</code><span>!</span></p>
<p><span>That</span>&rsquo;<span>s all for real for today, viva la vida!</span></p>
</section>
]]></content>
</entry>

<entry>
<title type="text">What is io_uring?</title>
<link href="https://matklad.github.io/2024/09/23/what-is-io-uring.html" rel="alternate" type="text/html" title="What is io_uring?" />
<published>2024-09-23T00:00:00+00:00</published>
<updated>2024-09-23T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/09/23/what-is-io-uring</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[An attempt at concise explanation of what io_uring is.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/09/23/what-is-io-uring.html"><![CDATA[
<h1><span>What is io_uring?</span> <time class="meta" datetime="2024-09-23">Sep 23, 2024</time></h1>
<p><span>An attempt at concise explanation of what io_uring is.</span></p>
<p><code>io_uring</code><span> is a new Linux kernel interface for making system calls.</span>
<span>Traditionally, syscalls are submitted to the kernel </span><strong><span>individually</span></strong><span> and</span>
<strong><span>synchronously</span></strong><span>: a syscall CPU instruction transfers control from the</span>
<span>application to the kernel; control returns to the application only when the</span>
<span>syscall is completed. In contrast, </span><code>io_uring</code><span> is a </span><strong><span>batched</span></strong><span> and </span><strong><span>asynchronous</span></strong>
<span>interface. The application submits several syscalls by writing their codes &amp;</span>
<span>arguments to a lock-free shared-memory ring buffer. The kernel reads the</span>
<span>syscalls from this shared memory and executes them at its own pace. To</span>
<span>communicate results back to the application, the kernel writes the results to a</span>
<span>second lock-free shared-memory ring buffer, where they become available to the</span>
<span>application asynchronously.</span></p>
<p><span>You might want to use </span><code>io_uring</code><span> if:</span></p>
<ul>
<li>
<span>you need extra performance unlocked by amortizing userspace/kernelspace</span>
<span>context switching across entire batches of syscalls,</span>
</li>
<li>
<span>you want a unified asynchronous interface to the entire system.</span>
</li>
</ul>
<p><span>You might want to avoid </span><code>io_uring</code><span> if:</span></p>
<ul>
<li>
<span>you need to write portable software,</span>
</li>
<li>
<span>you want to use only old, proven features,</span>
</li>
<li>
<span>and in particular you want to use features with a good security track record.</span>
</li>
</ul>
]]></content>
</entry>

<entry>
<title type="text">Try to Fix It One Level Deeper</title>
<link href="https://matklad.github.io/2024/09/06/fix-one-level-deeper.html" rel="alternate" type="text/html" title="Try to Fix It One Level Deeper" />
<published>2024-09-06T00:00:00+00:00</published>
<updated>2024-09-06T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/09/06/fix-one-level-deeper</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[I had a productive day today! I did many different and unrelated things, but they all had the same
unifying theme:]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/09/06/fix-one-level-deeper.html"><![CDATA[
<h1><span>Try to Fix It One Level Deeper</span> <time class="meta" datetime="2024-09-06">Sep 6, 2024</time></h1>
<p><span>I had a productive day today! I did many different and unrelated things, but they all had the same</span>
<span>unifying theme:</span></p>
<p><span>There</span>&rsquo;<span>s a bug! And it is sort-of obvious how to fix it. But if you don</span>&rsquo;<span>t laser-focus on that, and</span>
<span>try to perceive the surrounding context, it turns out that the bug is valuable, and it is pointing</span>
<span>in the direction of a bigger related problem. So, instead of fixing the bug directly, a detour is</span>
<span>warranted to close off the avenue for a class of bugs.</span></p>
<p><span>Here are the examples!</span></p>
<p><span>In the morning, my colleague pointed out that we are giving substandard error message for a pretty</span>
<span>stressful situation when the database runs out of disk space. I went ahead and added appropriate log</span>
<span>messages to make it clearer. But then I stopped for a moment and noticed that the problem is bigger</span>
&mdash;<span> we are missing an infrastructure for fatal errors, and </span><code>NoSpaceLeft</code><span> is just one of a kind. So I</span>
<span>went ahead and added that along the way:</span>
<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2289"><span>#2289</span></a><span>.</span></p>
<p><span>Then, I was reviewing a PR by </span><code>@martinconic</code><span> which was fixing some typos, and noticed that it was</span>
<span>also changing the formatting of our Go code. The latter is by far the biggest problem, as it is the</span>
<span>sign that we somehow are not running </span><code>gofmt</code><span> during our CI, which I fixed in</span>
<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2287"><span>#2287</span></a><span>.</span></p>
<p><span>Then, there was a PR from yesterday, where we again had a not quite right log message. The cause was</span>
<span>a confusion between two compile-time configuration parameters, which were close, but not quite</span>
<span>identical. So, instead of fixing the error message I went ahead and made the two parameters</span>
<em><span>exactly</span></em><span> the same. But then my colleague noticed that I actually failed to fix it one level deeper</span>
<span>in this case! Turns out, it is possible to remove this compile-time parametrization altogether,</span>
<span>which I did in </span><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2292"><span>#2292</span></a><span>.</span></p>
<p><span>But these all were randomly-generated side quests. My intended story line for today was to refactor</span>
<span>the piece of code I had trouble explaining (and understanding!) on </span><a href="https://www.youtube.com/watch?v=C3XAteN_lYk&amp;list=PL9eL-xg48OM3pnVqFSRyBFleHtBBw-nmZ&amp;index=41"><span>yesterday</span>&rsquo;<span>s</span>
<span>episode</span></a>
<span>of Iron Beetle. To get into the groove, I decided to first refactor the code that </span><em><span>calls</span></em><span> the</span>
<span>problematic piece of logic, as I noticed a couple of minor stylistic problems there. Of course, when</span>
<span>doing that, I discovered that we have a bit of dead code, which luckily doesn</span>&rsquo;<span>t affect correctness,</span>
<span>but does obscure the logic. While fixing that, I used one of my favorite Zig patterns:</span>
<span class="display"><code>defer assert(postcondition);</code></span></p>
<p><span>It of course failed in the simulator in a way postcondition checks tend to fail </span>&mdash;<span> there was an</span>
<span>unintended reentrancy in the code. So I slacked my colleague something like</span></p>

<figure class="blockquote">
<blockquote><p><span>I thought myself to be so clever adding this assert, but now it fails and I have to fix it TT</span>
<span>I think I</span>&rsquo;<span>ll just go and </span><code>.next_tick</code><span> the prefetch path. It feels like there should be a more</span>
<span>elegant solution here, but I am not seeing it.</span></p>
</blockquote>

</figure>
<p><span>But of course I can</span>&rsquo;<span>t just </span>&ldquo;<span>go and </span><code>.next_tick</code><span> it</span>&rdquo;<span>, so here I am, trying to figure out how to</span>
<span>encode a </span><a href="https://en.wikipedia.org/wiki/Duff%27s_device"><span>Duff</span>&rsquo;<span>s device</span></a><span> in Zig</span>
<span>pre-</span><a href="https://github.com/ziglang/zig/issues/8220"><span>#8220</span></a><span>, so as to make this class of issues much</span>
<span>less likely.</span></p>
]]></content>
</entry>

<entry>
<title type="text">The Fundamental Law Of Software Dependencies</title>
<link href="https://matklad.github.io/2024/09/03/the-fundamental-law-of-dependencies.html" rel="alternate" type="text/html" title="The Fundamental Law Of Software Dependencies" />
<published>2024-09-03T00:00:00+00:00</published>
<updated>2024-09-03T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/09/03/the-fundamental-law-of-dependencies</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[Canonical source code for software should include checksums of the content of all its
dependencies.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/09/03/the-fundamental-law-of-dependencies.html"><![CDATA[
<h1><span>The Fundamental Law Of Software Dependencies</span> <time class="meta" datetime="2024-09-03">Sep 3, 2024</time></h1>

<figure class="blockquote">
<blockquote><p><span>Canonical source code for software should include checksums of the content of </span><em><span>all</span></em><span> its</span>
<span>dependencies.</span></p>
</blockquote>

</figure>
<p><span>Several examples of the law:</span></p>
<p><span>Software obviously depends on its source code. The law says that </span><em><span>something</span></em><span> should hold the hash of</span>
<span>the entire source, and thus mandates the use of a content-addressed version control system such as</span>
<span>git.</span></p>
<p><span>Software often depends on 3rd party libraries. These libraries could in turn depend on </span><em><span>other</span></em>
<span>libraries. It is imperative to include a lockfile that covers this entire set and comes with</span>
<span>checksums. Curiously, the lockfile itself is a part of source code, and gets mixed into the VCS</span>
<span>root hash.</span></p>
<p><span>Software needs a compiler. The </span><em><span>hash</span></em><span> of the required compiler should be included in the lockfile.</span>
<span>Typically, this is not done </span>&mdash;<span> only the version is specified. I think that is a mistake. Specifying</span>
<span>a version and a hash is not much more trouble than just the version, but that gives you a superpower</span>
&mdash;<span> you no longer need to trust the party that distributes your compiler. You could take a shady</span>
<span>blob of bytes you</span>&rsquo;<span>ve found laying on the street, as long as its checksum checks out.</span></p>
<p><span>Note that you can compress hashes by mixing them. For compiler use-case, there</span>&rsquo;<span>s a separate hash per</span>
<span>platform, because the Linux and the Windows versions of the compiler differ. This doesn</span>&rsquo;<span>t mean that</span>
<span>your project should include one compiler</span>&rsquo;<span>s hash per platform, one hash is enough. Compiler</span>
<span>distribution should include a manifest </span>&ndash;<span> a small text file which lists all platform and their</span>
<span>platform specific hashes. The single hash of </span><em><span>that</span></em><span> file is what is to be included by downstream</span>
<span>consumers. To verify a specific binary, the consumer first downloads a manifest, checks that it</span>
<span>has the correct hash, and then extracts the hash for the specific platform.</span></p>
<hr>
<p><span>The law is an instrumental goal. By itself, hashes are not </span><em><span>that</span></em><span> useful. But to get to the point</span>
<span>where you actually </span><em><span>know</span></em><span> the hashes requires:</span></p>
<ul>
<li>
<span>Actually learning </span><em><span>what</span></em><span> are your dependencies (this is </span><em><span>not</span></em><span> trivial! If you have a single</span>
<span>Makefile or an </span><code>.sh</code><span>, you most likely don</span>&rsquo;<span>t know the set of your dependencies).</span>
</li>
<li>
<span>Coming up with some automated way to download those dependencies.</span>
</li>
<li>
<span>Fixing dependencies</span>&rsquo;<span>s build process to become reproducible, so as to have a meaningful hash at</span>
<span>all.</span>
</li>
<li>
<span>Learning to isolate dependencies per project, as hashed dependencies can</span>&rsquo;<span>t be installed into a</span>
<span>global shared namespace.</span>
</li>
</ul>
<p><em><span>These</span></em><span> things are what actually make developing software easier.</span></p>
]]></content>
</entry>

<entry>
<title type="text">STD Doesn't Have to Abstract OS IO</title>
<link href="https://matklad.github.io/2024/08/12/std-io.html" rel="alternate" type="text/html" title="STD Doesn't Have to Abstract OS IO" />
<published>2024-08-12T00:00:00+00:00</published>
<updated>2024-08-12T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/08/12/std-io</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[A short note on what goes into a language's standard library, and what's left for third party
libraries to implement!]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/08/12/std-io.html"><![CDATA[
<h1><span>STD Doesn</span>&rsquo;<span>t Have to Abstract OS IO</span> <time class="meta" datetime="2024-08-12">Aug 12, 2024</time></h1>
<p><span>A short note on what goes into a language</span>&rsquo;<span>s standard library, and what</span>&rsquo;<span>s left for third party</span>
<span>libraries to implement!</span></p>
<p><span>Usually, the main underlying driving factor here is cardinality. If it is important that there</span>&rsquo;<span>s</span>
<span>only one of a thing, it goes into std. If having many of a thing is a requirement, it is better</span>
<span>handled by a third-party library. That is, the usual physical constraint is that there</span>&rsquo;<span>s only a</span>
<em><span>single</span></em><span> standard library, and everyone uses the same standard library. In contrast, there are many</span>
<span>different third-party libraries, and they all can be used at the same time.</span></p>
<p><span>So, until very recently, my set of rules of thumb for what goes into stdlib looked roughly like</span>
<span>this:</span></p>
<ol>
<li>
<span>If this is a vocabulary type, which will be used by APIs of different libraries, it should be in</span>
<span>the stdlib.</span>
</li>
<li>
<span>If this is a cross platform abstraction around an IO facility provided by an OS, and this IO</span>
<span>facility has a reasonable common subset across most OSes, it should be in the stdlib.</span>
</li>
<li>
<span>If there</span>&rsquo;<span>s one obvious way to implement it, it might go to stdlib.</span>
</li>
</ol>
<p><span>So for example something like </span><a href="https://doc.rust-lang.org/stable/std/vec/struct.Vec.html"><code>Vec</code></a><span> goes</span>
<span>into a standard library, because all </span><em><span>other</span></em><span> libraries are going to use vectors at the interfaces.</span></p>
<p><span>Something like </span><a href="https://docs.rs/lazy_static/1.5.0/lazy_static/macro.lazy_static.html"><code>lazy_static</code></a>
<span>doesn</span>&rsquo;<span>t: while it is often needed, it is </span><em><span>not</span></em><span> a vocabulary interface type.</span></p>
<p><span>But it is acceptable for something like</span>
<a href="https://docs.rs/once_cell/1.19.0/once_cell/sync/struct.OnceCell.html"><code>OnceCell</code></a><span> to be in </span><code>std</code>
&mdash;<span> it is still not a vocabulary type, but, unlike </span><code>lazy_static</code><span>, it is clear that the API is more</span>
<span>or less optimal, and that there aren</span>&rsquo;<span>t that many good options to do this differently.</span></p>
<p><span>But I</span>&rsquo;<span>ve changed my mind about the second bullet point, about facilities like file IO or TCP</span>
<span>sockets. I was </span><em><span>always</span></em><span> under the impression that these things are a must for a standard library.</span>
<span>But now I think that</span>&rsquo;<span>s not necessarily true!</span></p>
<p><span>Consider randomness. Not the PRNG kind of randomness you</span>&rsquo;<span>d use to make a game fun, but a</span>
<span>cryptographically secure randomness that you</span>&rsquo;<span>d use to generate an SSH key pair. This sort of</span>
<span>randomness ultimately bottoms out in hardware, and fundamentally requires talking to the OS and</span>
<span>doing IO. This is squarely the bullet point number 2. And Rust is an interesting case study here: it</span>
<span>failed to provide this abstraction in std, even though std itself actually needs it! But this turned</span>
<span>out to be mostly a non-issue in practice </span>&mdash;<span> a third party crate, </span><code>getrandom</code><span>, took the job of</span>
<span>writing all the relevant bindings to various platform-specific API and using a bunch of conditional</span>
<span>compilation to abstract that all away and provide a nice cross-platform API.</span></p>
<p><span>So, no, it is not a </span><em><span>requirement</span></em><span> that std has to wrap any wrappable IOing API. This </span><em><span>could</span></em><span> be</span>
<span>handled by the library ecosystem, </span><em><span>if</span></em><span> the language allows first-class bindings to raw OS APIs</span>
<span>outside of compiler-privileged code (and Rust certainly allows for that).</span></p>
<p><span>So perhaps it won</span>&rsquo;<span>t be too unreasonable to leave even things like files and sockets to community</span>
<span>experimentation? In a sense, that is happening in the async land anyway.</span></p>
<hr>
<p><span>To clarify, I still believe that Rust </span><em><span>should</span></em><span> provide bindings to OS-sourced crypto randomness, and</span>
<span>I am extremely happy to see recent motion in that area. But the reason for this belief changed. I no</span>
<span>longer feel the mere fact that OS-specific APIs are involved to be particularly salient. However, it</span>
<span>is still true that there</span>&rsquo;<span>s more or less </span><a href="https://fuchsia.dev/reference/syscalls/cprng_draw"><span>one correct way to do</span>
<span>this</span></a><span>.</span></p>
]]></content>
</entry>

<entry>
<title type="text">Primitive Recursive Functions For A Working Programmer</title>
<link href="https://matklad.github.io/2024/08/01/primitive-recursive-functions.html" rel="alternate" type="text/html" title="Primitive Recursive Functions For A Working Programmer" />
<published>2024-08-01T00:00:00+00:00</published>
<updated>2024-08-01T00:00:00+00:00</updated>
<id>https://matklad.github.io/2024/08/01/primitive-recursive-functions</id>
<author><name>Alex Kladov</name></author>
<summary type="html"><![CDATA[Programmers on the internet often use Turing-completeness terminology. Typically, not being
Turing-complete is extolled as a virtue or even a requirement in specific domains. I claim that most
such discussions are misinformed --- that not being Turing complete doesn't actually mean what folks
want it to mean, and is instead a stand-in for a bunch of different practically useful properties,
which are mostly orthogonal to actual Turing completeness.]]></summary>
<content type="html" xml:base="https://matklad.github.io/2024/08/01/primitive-recursive-functions.html"><![CDATA[
<h1><span>Primitive Recursive Functions For A Working Programmer</span> <time class="meta" datetime="2024-08-01">Aug 1, 2024</time></h1>
<p><span>Programmers on the internet often use </span>&ldquo;<span>Turing-completeness</span>&rdquo;<span> terminology. Typically, not being</span>
<span>Turing-complete is extolled as a virtue or even a requirement in specific domains. I claim that most</span>
<span>such discussions are misinformed </span>&mdash;<span> that not being Turing complete doesn</span>&rsquo;<span>t actually mean what folks</span>
<span>want it to mean, and is instead a stand-in for a bunch of different practically useful properties,</span>
<span>which are mostly orthogonal to actual Turing completeness.</span></p>
<p><span>While I am generally descriptivist in nature and am ok with words losing their </span><em><span>original</span></em><span> meaning</span>
<span>as long as the new meaning is sufficiently commonly understood, Turing completeness is a hill I will</span>
<span>die on. It is a term from math, it has a very specific meaning, and you are not allowed to</span>
<span>re-purpose it for anything else, sorry!</span></p>
<p><span>I understand why this happens: to really understand what Turing completeness is and is not you need</span>
<span>to know one (simple!) theoretical result about so-called primitive recursive functions. And,</span>
<span>although this result is simple, I was only made aware of it in a fairly advanced course during my</span>
<span>masters. That</span>&rsquo;<span>s the CS education deficiency I want to rectify </span>&mdash;<span> you can</span>&rsquo;<span>t teach students the</span>
<span>halting problem without </span><em><span>also</span></em><span> teaching them about primitive recursion!</span></p>
<p><span>The post is going to be rather meaty, and will be split in three parts:</span></p>
<p><span>In Part I, I give a TL;DR for the theoretical result and some of its consequences. Part II is going</span>
<span>to be a whirlwind tour of Turing Machines, Finite State Automata and Primitive Recursive Functions.</span>
<span>And then Part III will circle back to practical matters.</span></p>
<p><span>If math makes you slightly nauseous, you might to skip Part II. But maybe give it a try? The math</span>
<span>we</span>&rsquo;<span>ll need will be baby math from first principles, without reference to any advanced results.</span></p>
<section id="Part-I-TL-DR">

    <h2>
    <a href="#Part-I-TL-DR"><span>Part I: TL;DR</span> </a>
    </h2>
<p><span>Here</span>&rsquo;<span>s the key result </span>&mdash;<span> suppose you have a program in some Turing complete language, and you also</span>
<span>know that it</span>&rsquo;<span>s not too slow. Suppose it runs faster than</span>
<span class="display"><span>O(2</span><sup><span>2</span><sup><span>N</span></sup></sup><span>).</span></span>
<span>That is, two to the power of two to the power of N, a very large number. In this case, you can</span>
<span>implement this algorithm in a non-Turing complete language.</span></p>
<p><span>Most practical problems fall into this </span>&ldquo;<span>faster than two to the two to the power of two</span>&rdquo;<span> space.</span>
<span>Hence it follows that you don</span>&rsquo;<span>t need the full power of a Turing Machine to tackle them. Hence, a</span>
<span>language not being Turing complete doesn</span>&rsquo;<span>t in any way restrict you in practice, or give you extra</span>
<span>powers to control the computation.</span></p>
<p><span>Or, to restate this: in practice, a program which doesn</span>&rsquo;<span>t terminate, and a program that needs a</span>
<span>billion billion steps to terminate are equivalent. Making something non-Turing complete by itself</span>
<span>doesn</span>&rsquo;<span>t help with the second problem in any way. And there</span>&rsquo;<span>s a trivial approach that solves the</span>
<span>first problem for any existing Turing-complete language </span>&mdash;<span> in the implementation, count the steps</span>
<span>and bail with an error after a billion.</span></p>
</section>
<section id="Part-II-Weird-Machines">

    <h2>
    <a href="#Part-II-Weird-Machines"><span>Part II: Weird Machines</span> </a>
    </h2>
<p><span>The actual theoretical result is quite a bit more general than that. It is (unsurprisingly)</span>
<span>recursive:</span></p>

<figure class="blockquote">
<blockquote><p><span>If a function is computed by a Turing Machine, and the runtime of this machine is bounded by some</span>
<span>primitive recursive function of input, then the original function itself can be written as a</span>
<span>primitive recursive function.</span></p>
</blockquote>

</figure>
<p><span>It is expected that this sounds like gibberish at this point! So let</span>&rsquo;<span>s just go and prove this thing,</span>
<span>right here in this blog post! Will work up slowly towards this result. The plan is as follows:</span></p>
<ul>
<li>
<em><span>First</span></em><span>, to brush up notation, we</span>&rsquo;<span>ll define Finite State Machines.</span>
</li>
<li>
<em><span>Second</span></em><span>, we</span>&rsquo;<span>ll turn our humble Finite State Machine into the all-powerful Turing Machine (spoiler</span>
&mdash;<span> a Turing Machine is an FSM with a pair of stacks), and, as is customary, wave our hands about</span>
<span>the Universal Turing Machine.</span>
</li>
<li>
<em><span>Third</span></em><span>, we leave the cozy world of imperative programming and define primitive recursive</span>
<span>functions.</span>
</li>
<li>
<em><span>Finally</span></em><span>, we</span>&rsquo;<span>ll talk about the relative computational power of TMs and PRFs, including the teased</span>
<span>up result and more!</span>
</li>
</ul>
</section>
<section id="Finite-State-Machines">

    <h2>
    <a href="#Finite-State-Machines"><span>Finite State Machines</span> </a>
    </h2>
<p><dfn><span>Finite State Machines</span></dfn><span> are simple! An FSM takes a string as input, and returns a binary</span>
<span>answer, </span>&ldquo;<span>yes</span>&rdquo;<span> or </span>&ldquo;<span>no</span>&rdquo;<span>. Unsurprisingly an FSM has a finite number of states: Q0, Q1, </span>&hellip;<span>, Qn.</span>
<span>A subset of states are designated as </span>&ldquo;<span>yes</span>&rdquo;<span> states, the rest are </span>&ldquo;<span>no</span>&rdquo;<span> states. There</span>&rsquo;<span>s also one</span>
<span>specific starting state.</span></p>
<p><span>The behavior of the state machine is guided by a transition (step) function, </span><code>s</code><span>. This function</span>
<span>takes the current state of FSM, the next symbol of input, and returns a new state.</span></p>
<p><span>The semantics of FSM is determined by repeatably applying the single step function for all symbols of</span>
<span>the input, and noting whether the final state is a </span>&ldquo;<span>yes</span>&rdquo;<span> state or a </span>&ldquo;<span>no</span>&rdquo;<span> state.</span></p>
<p><span>Here</span>&rsquo;<span>s an FSM which accepts only strings of zeros and ones of even length:</span></p>

<figure class="code-block">


<pre><code><span class="line">States:     { Q0, Q1 }</span>
<span class="line">Yes States: { Q0 }</span>
<span class="line">Start State:  Q0</span>
<span class="line"></span>
<span class="line">s :: State -&gt; Symbol -&gt; State</span>
<span class="line">s Q0 0 = Q1</span>
<span class="line">s Q0 1 = Q1</span>
<span class="line">s Q1 0 = Q0</span>
<span class="line">s Q1 1 = Q0</span></code></pre>

</figure>
<p><span>This machine ping-pongs between states Q0 and Q1 ends up in Q0 only for inputs of even length</span>
<span>(including an empty input).</span></p>
<p><span>What can FSMs do? As they give a binary answer, they are recognizers </span>&mdash;<span> they don</span>&rsquo;<span>t compute</span>
<span>functions, but rather just characterize certain sets of strings. A famous result is that the</span>
<span>expressive power of FSMs is equivalent to the expressive power of regular expressions. If you can</span>
<span>write a regular expression for it, you could also do an FSM!</span></p>
<p><span>There are also certain things that state machines can</span>&rsquo;<span>t do. For example they can</span>&rsquo;<span>t enter an infinite</span>
<span>loop. Any FSM is linear in the input size and always terminates. But there are much more specific</span>
<span>sets of strings that couldn</span>&rsquo;<span>t be recognized by an FSM. Consider this set:</span></p>

<figure class="code-block">


<pre><code><span class="line">1</span>
<span class="line">010</span>
<span class="line">00100</span>
<span class="line">0001000</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>That is, an infinite set which contains </span>&lsquo;<span>1</span>&rsquo;<span>s surrounded by the equal number of </span>&lsquo;<span>0</span>&rsquo;<span>s on the both</span>
<span>sides. Let</span>&rsquo;<span>s prove that there isn</span>&rsquo;<span>t a state machine that recognizes this set!</span></p>
<p><span>As usually, suppose there </span><em><span>is</span></em><span> such a state machine. It has a certain number of states </span>&mdash;<span> maybe a</span>
<span>dozen, maybe a hundred, maybe a thousand, maybe even more. But let</span>&rsquo;<span>s say fewer than a million.</span>
<span>Then, let</span>&rsquo;<span>s take a string which looks like a million zeros, followed by one, followed by million</span>
<span>zeros. And let</span>&rsquo;<span>s observe our FSM eating this particular string.</span></p>
<p><span>First of all, because the string is in fact a one surrounded by the equal number of zeros on both</span>
<span>sides, the FSM ends up in a </span>&ldquo;<span>yes</span>&rdquo;<span> state. Moreover, because the length of the string is much greater</span>
<span>than the number of states in the state machine, the state machine necessarily visits some state twice.</span>
<span>There is a cycle, where the machine goes from A to B to C to D and back to A. This cycle might be</span>
<span>pretty long, but it</span>&rsquo;<span>s definitely shorter than the total number of states we have.</span></p>
<p><span>And now we can fool the state machine. Let</span>&rsquo;<span>s make it eat our string again, but this time, once it</span>
<span>completes the ABCDA cycle, we</span>&rsquo;<span>ll force it to traverse this cycle again. That is, the original cycle</span>
<span>corresponds to some portion of our giant string:</span></p>

<figure class="code-block">


<pre><code><span class="line">0000 0000000000000000000 00 .... 1 .... 00000</span>
<span class="line">     &lt;- cycle portion -&gt;</span></code></pre>

</figure>
<p><span>If we duplicate this portion, our string will no longer look like one surrounded by equal number of</span>
<span>twos, but the state machine will still in the </span>&ldquo;<span>yes</span>&rdquo;<span> state. Which is a contradiction that completes</span>
<span>the proof.</span></p>
</section>
<section id="Turing-Machine-Definition">

    <h2>
    <a href="#Turing-Machine-Definition"><span>Turing Machine: Definition</span> </a>
    </h2>
<p><span>A </span><dfn><span>Turing Machine</span></dfn><span> is only slightly more complex than an FSM. Like an FSM, a TM has a bunch of states</span>
<span>and a single-step transition function. While an FSM has an immutable input which is being fed to it</span>
<span>symbol by symbol, a TM operates with a mutable tape. The input gets written to the tape at the</span>
<span>start. At each step, a TM looks at the current symbol on the tape, changes its state according to a</span>
<span>transition function and, additionally:</span></p>
<ul>
<li>
<span>Replaces the current symbol with a new one (which might or might not be different).</span>
</li>
<li>
<span>Moves the reading head that points at the current symbol one position to the left or to the right.</span>
</li>
</ul>
<p><span>When a machine reaches a designated halt state, it stops, and whatever is written on the tape at</span>
<span>that moment is the result. That is, while FSMs are binary recognizers, TMs are functions. Keep in</span>
<span>mind that a TM does not necessarily stop. It might be the case that a TM goes back and forth over the</span>
<span>tape, overwrites it, changes its internal state, but never quite gets to the final state.</span></p>
<p><span>Here</span>&rsquo;<span>s an example Turing Machine:</span></p>

<figure class="code-block">


<pre><code><span class="line">States:  {A, B, C, H}</span>
<span class="line">Start State: A</span>
<span class="line">Final State: H</span>
<span class="line"></span>
<span class="line">s :: State -&gt; Symbol -&gt; (State, Symbol, Left | Right)</span>
<span class="line">s A 0 = (B, 1, Right)</span>
<span class="line">s A 1 = (H, 1, Right)</span>
<span class="line">s B 0 = (C, 0, Right)</span>
<span class="line">s B 1 = (B, 1, Right)</span>
<span class="line">s C 0 = (C, 1, Left)</span>
<span class="line">s C 1 = (A, 1, Left)</span></code></pre>

</figure>
<p><span>If the configuration of the machine looks like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">000010100000</span>
<span class="line">     ^</span>
<span class="line">     A</span></code></pre>

</figure>
<p><span>Then we are in the </span><code>s A 0 = (B, 1, Right)</code><span> case, so we should change the state to B, replace 0 with</span>
<span>1, and move to the right:</span></p>

<figure class="code-block">


<pre><code><span class="line">000011100000</span>
<span class="line">      ^</span>
<span class="line">      B</span></code></pre>

</figure>
</section>
<section id="Turing-Machine-Programming">

    <h2>
    <a href="#Turing-Machine-Programming"><span>Turing Machine: Programming</span> </a>
    </h2>
<p><span>There are a bunch of fiddly details to Turing Machines!</span></p>
<p><span>The tape is conceptually infinite, so beyond the input, everything is just zeros. This creates a</span>
<span>problem: it might be hard to say where the input (or the output) ends! There are a couple of</span>
<span>technical solutions here. One is to say that there are three different symbols on the tape </span>&mdash;
<span>zeros, ones, and blanks, and require that the tape is initialized with blanks. A different solution</span>
<span>is to invent some encoding scheme. For example, we can say that the input is a sequence of 8-bit</span>
<span>bytes, without interior null bytes. So, eight consecutive zeros at a byte boundary designate the end</span>
<span>of input/output.</span></p>
<p><span>It</span>&rsquo;<span>s useful to think about how this byte-oriented TM could be implemented. We could have one large</span>
<span>state for each byte of input. So, Q142 would mean that the head is on the byte with value 142. And</span>
<span>then we</span>&rsquo;<span>ll have a bunch of small states to read out the current byte. Eg, we start reading a byte in</span>
<span>state </span><code>S</code><span>. Depending on the next bit we move to S0 or S1, then to S00, or S01, etc. Once we reached</span>
<span>something like S01111001, we move back 8 positions and enter state Q121. This is one of the patterns</span>
<span>of Turing Machine programming </span>&mdash;<span> while your main memory is the tape, you can represent some</span>
<span>constant amount of memory directly in the states.</span></p>
<p><span>What we</span>&rsquo;<span>ve done here is essentially lowering a byte-oriented Turing Machine to a bit-oriented</span>
<span>machine. So, we could think only in terms of big states operating on bytes, as we know the general</span>
<span>pattern for converting that to direct bit-twiddling.</span></p>
<p><span>With this encoding scheme in place, we now can feed arbitrary files to a Turing Machine! Which will</span>
<span>be handy to the next observation:</span></p>
<p><span>You can</span>&rsquo;<span>t actually program a Turing Machine. What I mean is that, counter-intuitively, there isn</span>&rsquo;<span>t</span>
<span>some user-supplied program that a Turing Machine executes. Rather, the program is hard-wired into</span>
<span>the machine. The transition function </span><em><span>is</span></em><span> the program.</span></p>
<p><span>But with some ingenuity we can regain our ability to write programs. Recall that we</span>&rsquo;<span>ve just learned</span>
<span>to feed arbitrary files to a TM. So what we could do is to write a text file that specifies a TM and</span>
<span>its input, and then feed that entire file as an input to an </span>&ldquo;<span>interpreter</span>&rdquo;<span> Turing Machine which would</span>
<span>read the file, and act as the machine specified there. A Turing Machine can have an </span><code>eval</code>
<span>function.</span></p>
<p><span>Is such an </span>&ldquo;<span>interpreter</span>&rdquo;<span> Turing Machine possible? Yes! And it is not hard: if you spend a couple of hours</span>
<span>programming Turing Machines by hand, you</span>&rsquo;<span>ll see that you pretty much can do anything </span>&mdash;<span> you can do</span>
<span>numbers, arithmetic, loops, control flow. It</span>&rsquo;<span>s just very very tedious.</span></p>
<p><span>So let</span>&rsquo;<span>s just declare that we</span>&rsquo;<span>ve actually coded up this Universal Turing Machine which simulates a</span>
<span>TM given to it as an input in a particular encoding.</span></p>
<p><span>This sort of construct also gives rise to the Church-Turing thesis. We have a TM which can run other</span>
<span>TMs. And you can implement a TM interpreter in something like Python. And, with a bit of legwork,</span>
<span>you could </span><em><span>also</span></em><span> implement a Python interpreter as a TM (you likely want to avoid doing that</span>
<span>directly, and instead do a simpler interpreter for WASM, and then use a Python interpreter compiled</span>
<span>to WASM). This sort of bidirectional interpretation shows that Python and TMs have equivalent</span>
<span>computing power. Moreover, it</span>&rsquo;<span>s quite hard to come up with a reasonable computational device which</span>
<span>is more powerful than a Turing Machine.</span></p>
<p><span>There are computational devices that are strictly weaker than TMs though. Recall FSMs. By this point,</span>
<span>it should be obvious that a TM can simulate an FSM. Everything a Finite State Machine can do, a</span>
<span>Turing Machine can do as well. And it should be intuitively clear that a TM is more powerful than an</span>
<span>FSM. An FSM gets to use only a finite number of states. A TM has these same states, but it also posses</span>
<span>a tape which serves like an infinitely sized external memory.</span></p>
<p><span>Directly proving that you </span><em><span>can</span>&rsquo;<span>t</span></em><span> encode a Universal Turing Machine as an FSM sounds complicated,</span>
<span>so let</span>&rsquo;<span>s prove something simpler. Recall that we have established that there</span>&rsquo;<span>s no FSM that accepts</span>
<span>only ones surrounded by an equal number of zeros on both sides (because a sufficiently large word</span>
<span>of this form would necessary enter a cycle in a state machine, which could then be further pumped).</span>
<span>But it</span>&rsquo;<span>s actually easy to write a Turing Machine that does this:</span></p>
<ul>
<li>
<span>Erase zero (at the left side of the tape)</span>
</li>
<li>
<span>Go to the right end of the tape</span>
</li>
<li>
<span>Erase zero</span>
</li>
<li>
<span>Go to the left side of the tape</span>
</li>
<li>
<span>Repeat</span>
</li>
<li>
<span>If what</span>&rsquo;<span>s left is a single </span><code>1</code><span> the answer is </span>&ldquo;<span>yes</span>&rdquo;<span>, otherwise it is a </span>&ldquo;<span>no</span>&rdquo;
</li>
</ul>
<p><span>We found a specific problem that can be solved by a TM, but is out of reach of any FSM. So it</span>
<span>necessarily follows that there isn</span>&rsquo;<span>t an FSM that can simulate an arbitrary TM.</span></p>
<p><span>It is also useful to take a closer look at the tape. It is a convenient skeuomorphic abstraction</span>
<span>which makes the behavior of the machine intuitive, but it is inconvenient to implement in a normal</span>
<span>programming language. There isn</span>&rsquo;<span>t a standard data structure that behaves just like a tape.</span></p>
<p><span>One cool practical trick is to simulate the tape as a pair of stacks. Take this:</span></p>

<figure class="code-block">


<pre><code><span class="line">Tape: A B C D E F G</span>
<span class="line">Head:     ^</span></code></pre>

</figure>
<p><span>And transform it to something like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">Left Stack:  [A, B, C]</span>
<span class="line">Right Stack: [G, F, E, D]</span></code></pre>

</figure>
<p><span>That is, everything to the left of the head is one stack, everything to the right, reversed, is the</span>
<span>other.  Here, moving the reading head left or right corresponds to popping a value off one stack and</span>
<span>pushing it onto another.</span></p>
<p><span>So, an equivalent-in-power definition would be to say that a TM is an FSM endowed with two</span>
<span>stacks.</span></p>
<p><span>This of course creates an obvious question: is an FSM with just one stack a thing? Yes! It would be</span>
<span>called a pushdown automaton, and it would correspond to context-free languages. But that</span>&rsquo;<span>s beyond</span>
<span>the scope of this post!</span></p>
<p><span>There</span>&rsquo;<span>s yet another way to look at the tape, or the pair of stacks, if the set of symbols is 0 and</span>
<span>1. You could say that a stack is just a number! So, something like</span>
<code class="display">[1, 0, 1, 1]</code>
<span>will be</span>
<span class="display"><code>1 + 2 + 8 = 11</code><span>.</span></span>
<span>Looking at the top of the stack is </span><code>stack % 2</code><span>, removing an item from the stack is </span><code>stack / 2</code><span> and</span>
<span>pushing x onto the stack is </span><code>stack * 2 + x</code><span>. We won</span>&rsquo;<span>t need this </span><em><span>right</span></em><span> now, so just hold onto this</span>
<span>for a brief moment.</span></p>
</section>
<section id="Turing-Machine-Limits">

    <h2>
    <a href="#Turing-Machine-Limits"><span>Turing Machine: Limits</span> </a>
    </h2>
<p><span>Ok, so we have some idea about the lower bound for the power of a Turing Machine </span>&mdash;<span> FSMs are strictly</span>
<span>less expressive. What about the opposite direction? Is there some computation that a Turing Machine</span>
<span>is incapable of doing?</span></p>
<p><span>Yes! Let</span>&rsquo;<span>s construct a function which maps natural numbers to natural numbers, which can</span>&rsquo;<span>t be</span>
<span>implemented by a Turing Machine. Recall that we can encode an arbitrary Turing Machine as text. That</span>
<span>means that we can actually enumerate all possible Turing Machines, and write them in a giant line,</span>
<span>from the most simple Turing Machine to more complex ones:</span></p>

<figure class="code-block">


<pre><code><span class="line">TM_0</span>
<span class="line">TM_1</span>
<span class="line">TM_2</span>
<span class="line">...</span>
<span class="line">TM_326</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>This is of course going to be an infinite list.</span></p>
<p><span>Now, let</span>&rsquo;<span>s see how TM0 behaves on input </span><code>0</code><span>: it either prints something, or doesn</span>&rsquo;<span>t terminate. Then,</span>
<span>note how TM1 behaves on input </span><code>1</code><span>, and generalizing, create function </span><code>f</code><span> that behaves as the nth TM</span>
<span>on input </span><code>n</code><span>. It might look something like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(0) = 0</span>
<span class="line">f(1) = 111011</span>
<span class="line">f(2) = doesn't terminate</span>
<span class="line">f(3) = 0</span>
<span class="line">f(4) = 101</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>Now, let</span>&rsquo;<span>s construct function </span><code>g</code><span> which is maximally diffed from </span><code>f</code><span>: where </span><code>f</code><span> gives </span><code>0</code><span>, </span><code>g</code><span> will</span>
<span>return </span><code>1</code><span>, and it will return </span><code>0</code><span> in all other cases:</span></p>

<figure class="code-block">


<pre><code><span class="line">g(0) = 1</span>
<span class="line">g(1) = 0</span>
<span class="line">g(2) = 0</span>
<span class="line">g(3) = 1</span>
<span class="line">g(4) = 0</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>There isn</span>&rsquo;<span>t a Turing machine that computes </span><code>g</code><span>. For suppose there is. Then, it exists in our list of</span>
<span>all Turing Machines somewhere. Let</span>&rsquo;<span>s say it is TM1000064. So, if we feed </span><code>0</code><span> to it, it will return</span>
<code>g(0)</code><span>, which is </span><code>1</code><span>, which is different from </span><code>f(0)</code><span>. And the same holds for </span><code>1</code><span>, and </span><code>2</code><span>, and </span><code>3</code><span>.</span>
<span>But once we get to </span><code>g(1000064)</code><span>, we are in trouble, because, by the definition of </span><code>g</code><span>, </span><code>g(1000064)</code>
<span>is different from what is computed by TM1000064! So such a machine is impossible.</span></p>
<p><span>Those math savvy might express this more succinctly </span>&mdash;<span> there</span>&rsquo;<span>s a countably-infinite number of</span>
<span>Turing Machines, and an uncountably-infinite number of functions. So there </span><em><span>must</span></em><span> be some functions</span>
<span>which do not have a corresponding Turing Machine. It is the same proof </span>&mdash;<span> the diagonalization</span>
<span>argument is hiding in the claim that the set of all functions is an uncountable set.</span></p>
<p><span>But this is super weird and abstract. Let</span>&rsquo;<span>s rather come up with some very specific problem which</span>
<span>isn</span>&rsquo;<span>t solvable by a Turing Machine. The halting problem: given source code for a Turing Machine and</span>
<span>its input, determine if the machine halts on this input eventually.</span></p>
<p><span>As we have waved our hands sufficiently vigorously to establish that Python and Turing Machines have</span>
<span>equivalent computational power, I am going to try to solve this in Python:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts</span>(<span class="hl-params">program_source_code: <span class="hl-built_in">str</span>, program_input: <span class="hl-built_in">str</span></span>) -&gt; Bool:</span>
<span class="line">    <span class="hl-comment"># One million lines of readable, but somewhat</span></span>
<span class="line">    <span class="hl-comment"># unsettling and intimidating Python code.</span></span>
<span class="line">    <span class="hl-keyword">return</span> the_answer</span>
<span class="line"></span>
<span class="line">raw_input = <span class="hl-built_in">input</span>()</span>
<span class="line">[program_source_code, program_input] = parse(raw_input)</span>
<span class="line"><span class="hl-built_in">print</span>(<span class="hl-string">&quot;Yes&quot;</span> <span class="hl-keyword">if</span> halts(program_source_code, program_input) <span class="hl-keyword">else</span> <span class="hl-string">&quot;No&quot;</span>)</span></code></pre>

</figure>
<p><span>Now, I will do a weird thing and start asking whether a program terminates, if it is fed its own</span>
<span>source code, in a reverse-quine of sorts:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts_on_self</span>(<span class="hl-params">program_source_code: <span class="hl-built_in">str</span></span>) -&gt; Bool:</span>
<span class="line">    program_input = program_source_code</span>
<span class="line">    <span class="hl-keyword">return</span> halts(program_source_code, program_input)</span></code></pre>

</figure>
<p><span>and finally I construct this weird beast of a program:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts</span>(<span class="hl-params">program_source_code: <span class="hl-built_in">str</span>, program_input: <span class="hl-built_in">str</span></span>) -&gt; Bool:</span>
<span class="line">    <span class="hl-comment"># ...</span></span>
<span class="line">    <span class="hl-keyword">return</span> the_answer</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts_on_self</span>(<span class="hl-params">program_source_code: <span class="hl-built_in">str</span></span>) -&gt; Bool:</span>
<span class="line">    program_input = program_source_code</span>
<span class="line">    <span class="hl-keyword">return</span> halts(program_source_code, program_input)</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">weird</span>(<span class="hl-params">program_input</span>):</span>
<span class="line">    <span class="hl-keyword">if</span> halts_on_self(program_input):</span>
<span class="line">        <span class="hl-keyword">while</span> <span class="hl-literal">True</span>:</span>
<span class="line">            <span class="hl-keyword">pass</span></span>
<span class="line"></span>
<span class="line">weird(<span class="hl-built_in">input</span>())</span></code></pre>

</figure>
<p><span>To make this even worse, I</span>&rsquo;<span>ll feed the text of this </span><code>weird</code><span> program to itself. Does it terminate</span>
<span>with this input? Well, if it terminates, and if our </span><code>halts</code><span> function is implemented correctly, then</span>
<span>the </span><code>halts_on_self(program_input)</code><span> invocation above returns </span><code>True</code><span>. But then we enter the infinite</span>
<span>loop and don</span>&rsquo;<span>t actually terminate.</span></p>
<p><span>Hence, it must be the case that </span><code>weird</code><span> does not terminate when self-applied. But then</span>
<code>halts_on_self</code><span> returns </span><code>False</code><span>, and it should terminate. So we get a contradiction both ways. Which</span>
<span>necessarily means that either our </span><code>halts</code><span> sometimes returns a straight-up incorrect answer, or that it</span>
<span>sometimes does not terminate.</span></p>
<p><span>So this is the flip side of a Turing Machine</span>&rsquo;<span>s power </span>&mdash;<span> it is so powerful that it becomes impossible</span>
<span>to tell whether it</span>&rsquo;<span>ll terminate or not!</span></p>
<p><span>It actually gets much worse, because this result can be generalized to an unreasonable degree!</span>
<span>In general, there</span>&rsquo;<span>s very little we can say about arbitrary programs.</span></p>
<p><span>We can easily check syntactic properties (is the program text shorter than 4 kilobytes?), but they</span>
<span>are, in some sense, not very interesting, as they depend a lot on how exactly one writes a program.</span>
<span>It would be much more interesting to check some refactoring-invariant properties, which hold when</span>
<span>you change the text of the program, but leave the behavior intact. Indeed, </span>&ldquo;<span>does this change</span>
<span>preserve behavior?</span>&rdquo;<span> would be one very useful property to check!</span></p>
<p><span>So let</span>&rsquo;<span>s define two TMs to be equivalent, if they have identical behavior. That is, for each</span>
<span>specific input, either both machines don</span>&rsquo;<span>t terminate, or they both halt, and give identical results.</span></p>
<p><span>Then, our refactoring-invariant properties are, by definition, properties that hold (or do not hold)</span>
<span>for the entire classes of equivalence of TMs.</span></p>
<p><span>And a somewhat depressing result here is that there are no non-trivial refactoring-invariant</span>
<span>properties that you can algorithmically check.</span></p>
<p><span>Suppose we have some magic TM, called P, which checks such a property. Let</span>&rsquo;<span>s show that, using P, we can</span>
<span>solve the problem we know we can not solve </span>&mdash;<span> the halting problem.</span></p>
<p><span>Consider a Turing Machine that is just an infinite loop and never terminates, M1. P might or might</span>
<span>not hold for it. But, because P is non-trivial (it holds for some machines and doesn</span>&rsquo;<span>t hold for some</span>
<span>machines), there</span>&rsquo;<span>s some different machine M2 which differs from M1 with respect to P. That is,</span>
<code>P(M1) xor P(M2)</code><span> holds.</span></p>
<p><span>Let</span>&rsquo;<span>s use these M1 and M2 to figure out whether a given machine M halts on input I. Using Universal</span>
<span>Turing Machine (interpreter), we can construct a new machine, M12 that just runs M on input I, then</span>
<span>erases the contents of the tape and runs M2. Now, if M halts on I, then the resulting machine M12 is</span>
<span>behaviorally-equivalent to M2. If M doesn</span>&rsquo;<span>t halt on I, then the result is equivalent to the infinite</span>
<span>loop program, M1. Or, in pseudo-code:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">M1</span>(<span class="hl-params"><span class="hl-built_in">input</span></span>):</span>
<span class="line">    <span class="hl-keyword">while</span> <span class="hl-literal">True</span>:</span>
<span class="line">        <span class="hl-keyword">pass</span></span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">M2</span>(<span class="hl-params"><span class="hl-built_in">input</span></span>):</span>
<span class="line">    <span class="hl-comment"># We don&#x27;t actually know what&#x27;s here</span></span>
<span class="line">    <span class="hl-comment"># but we know that such a machine exists.</span></span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">assert</span>(P(M1) != P(M2))</span>
<span class="line"></span>
<span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">halts</span>(<span class="hl-params">M, I</span>):</span>
<span class="line">    <span class="hl-keyword">def</span> <span class="hl-title function_">M12</span>(<span class="hl-params"><span class="hl-built_in">input</span></span>):</span>
<span class="line">        M(I) <span class="hl-comment"># might or might not halt</span></span>
<span class="line">        <span class="hl-keyword">return</span> M2(<span class="hl-built_in">input</span>)</span>
<span class="line"></span>
<span class="line">    <span class="hl-keyword">return</span> P(M12) == P(M2)</span></code></pre>

</figure>
<p><span>This is pretty bad and depressing </span>&mdash;<span> we can</span>&rsquo;<span>t learn anything meaningful about an arbitrary Turing</span>
<span>Machine! So let</span>&rsquo;<span>s finally get to the actual topic of today</span>&rsquo;<span>s post:</span></p>
</section>
<section id="Primitive-Recursive-Functions">

    <h2>
    <a href="#Primitive-Recursive-Functions"><span>Primitive Recursive Functions</span> </a>
    </h2>
<p><span>This is going to be another computational device, like FSMs and TMs. Like an FSM, it</span>&rsquo;<span>s going to be a</span>
<span>nice, always terminating, non-Turing complete device. But it will turn out to have quite a bit of</span>
<span>the power of a full Turing Machine!</span></p>
<p><span>However, unlike both TMs and FSMs, </span><dfn><span>Primitive Recursive Functions</span></dfn><span> are defined directly as</span>
<span>functions which take a tuple of natural numbers and return a natural number. The two simplest ones</span>
<span>are </span><code>zero</code><span> (that is, zero-arity function that returns </span><code>0</code><span>) and </span><code>succ</code><span> </span>&mdash;<span> a unary function that</span>
<span>just adds 1. Everything else is going to get constructed out of these two:</span></p>

<figure class="code-block">


<pre><code><span class="line">zero = 0</span>
<span class="line">succ(x) = x + 1</span></code></pre>

</figure>
<p><span>One way we are allowed to combine these functions is by composition. So we can get all the constants</span>
<span>right off the bat:</span></p>

<figure class="code-block">


<pre><code><span class="line">succ(zero) = 1</span>
<span class="line">succ(succ(zero)) = 2</span>
<span class="line">succ(succ(succ(zero))) = 3</span></code></pre>

</figure>
<p><span>We aren</span>&rsquo;<span>t going to be allowed to use general recursion (because it can trivially non-terminate),</span>
<span>but we do get to use a restricted form of C-style loop. It is a bit fiddly to define formally! The</span>
<span>overall shape is </span><span class="display"><code>LOOP(init, f, n)</code><span>.</span></span></p>
<p><span>Here, </span><code>init</code><span> and </span><code>n</code><span> are numbers </span>&mdash;<span> the initial value of the accumulator and the total number of</span>
<span>iterations. The </span><code>f</code><span> is a unary function that specifies the loop body </span>&ndash;<span> it takes the current value</span>
<span>of the accumulator and returns the new value. So</span></p>

<figure class="code-block">


<pre><code><span class="line">LOOP(init, f, 0) = init</span>
<span class="line">LOOP(init, f, 1) = f(init)</span>
<span class="line">LOOP(init, f, 2) = f(f(init))</span>
<span class="line">LOOP(init, f, 3) = f(f(f(init)))</span></code></pre>

</figure>
<p><span>While this is </span><em><span>similar</span></em><span> to a C-style loop, the crucial difference here is that the total number of</span>
<span>iterations </span><code>n</code><span> is fixed up-front. There</span>&rsquo;<span>s no way to mutate the loop counter in the loop body.</span></p>
<p><span>This allows us to define addition:</span></p>

<figure class="code-block">


<pre><code><span class="line">add(x, y) = LOOP(x, succ, y)</span></code></pre>

</figure>
<p><span>Multiplication is trickier. Conceptually, to multiply </span><code>x</code><span> and </span><code>y</code><span>, we want to </span><code>LOOP</code><span> from zero, and</span>
<span>repeat </span>&ldquo;<span>add </span><code>x</code>&rdquo;<span> </span><code>y</code><span> times. The problem here is that we can</span>&rsquo;<span>t write an </span>&ldquo;<span>add </span><code>x</code>&rdquo;<span> function yet</span></p>

<figure class="code-block">


<pre><code><span class="line"># Doesn't work, add is a binary function!</span>
<span class="line">mul(x, y) = LOOP(0, add, y)</span></code></pre>

</figure>

<figure class="code-block">


<pre><code><span class="line"># Doesn't work either, no x in scope!</span>
<span class="line">add_x v = add(x, v)</span>
<span class="line">mul(x, y) = LOOP(0, add_x, y)</span></code></pre>

</figure>
<p><span>One way around this is to define </span><code>LOOP</code><span> as a family of operators, which can pass extra arguments to</span>
<span>the iteration function:</span></p>

<figure class="code-block">


<pre><code><span class="line">LOOP0(init, f, 2) = f(f(init))</span>
<span class="line">LOOP1(c1, init, f, 2) = f(c1, f(c1, init))</span>
<span class="line">LOOP2(c1, c2, init, f, 2) = f(c1, c2, f(c1, c2, init))</span></code></pre>

</figure>
<p><span>That is, </span><code>LOOP_N</code><span> takes an extra </span><code>n</code><span> arguments, and passes them through to any invocation of the body</span>
<span>function. To express this idea a little bit more succinctly, let</span>&rsquo;<span>s just allow to partially  apply</span>
<span>the second argument of </span><code>LOOP</code><span>. That is:</span></p>
<ul>
<li>
<span>All our functions are going to be first order. All arguments are numbers, the result is a number.</span>
<span>There aren</span>&rsquo;<span>t higher order functions, there aren</span>&rsquo;<span>t closures.</span>
</li>
<li>
<span>The </span><code>LOOP</code><span> is not a function in our language </span>&mdash;<span> it</span>&rsquo;<span>s a builtin operator, a keyword. So, for</span>
<span>convenience, we allow passing partially applied functions to it. But semantically this is</span>
<span>equivalent to just passing in extra arguments on each iteration.</span>
</li>
</ul>
<p><span>Which finally allows us to write</span></p>

<figure class="code-block">


<pre><code><span class="line">mul(x, y) = LOOP(0, add x, y)</span></code></pre>

</figure>
<p><span>Ok, so that</span>&rsquo;<span>s progress </span>&mdash;<span> we made something as complicated as multiplication, and we still are in</span>
<span>the guaranteed-to-terminate land. Because each loop has a fixed number of iterations, everything</span>
<span>eventually finishes.</span></p>
<p><span>We can go on and define x</span><sup><span>y</span></sup><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line">pow(x, y) = LOOP(1, mul x, y)</span></code></pre>

</figure>
<p><span>And this in turn allows us to define a couple of concerning fast growing functions:</span></p>

<figure class="code-block">


<pre><code><span class="line">pow_2(n) = pow(2, n)</span>
<span class="line">pow_2_2(n) = pow_2(pow_2(n))</span></code></pre>

</figure>
<p><span>That</span>&rsquo;<span>s fun, but to do some programming, we</span>&rsquo;<span>ll need an </span><code>if</code><span>. We</span>&rsquo;<span>ll get to it, but first we</span>&rsquo;<span>ll need</span>
<span>some boolean operations. We can encode </span><code>false</code><span> as </span><code>0</code><span> and </span><code>true</code><span> as </span><code>1</code><span>. Then</span></p>

<figure class="code-block">


<pre><code><span class="line">and(x, y) = mul(x, y)</span></code></pre>

</figure>
<p><span>But </span><code>or</code><span> creates a problem: we</span>&rsquo;<span>ll need a subtraction.</span></p>

<figure class="code-block">


<pre><code><span class="line">or(x, y) = sub(</span>
<span class="line">  add(x, y),</span>
<span class="line">  mul(x, y),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>Defining </span><code>sub</code><span> is tricky, due to two problems:</span></p>
<p><span>First, we only have natural numbers, no negatives. This one is easy to solve </span>&mdash;<span> we</span>&rsquo;<span>ll just define</span>
<span>subtraction to saturate.</span></p>
<p><span>The second problem is more severe </span>&mdash;<span> I think we actually can</span>&rsquo;<span>t express subtraction given the set of</span>
<span>allowable operations so far. That is because all our operations are monotonic </span>&mdash;<span> the result is</span>
<span>never less than the arguments. One way to solve this problem is to define the LOOP in such a way</span>
<span>that the body function also gets passed a second argument </span>&mdash;<span> the current iteration. So, if you</span>
<span>iterate up to </span><code>n</code><span>, the last iteration will observe </span><code>n - 1</code><span>, and that would be the non-monotonic</span>
<span>operation that creates subtraction. But that seems somewhat inelegant to me, so instead I will just</span>
<span>add a </span><code>pred</code><span> function to the basis, and use that to add loop counters to our iterations.</span></p>

<figure class="code-block">


<pre><code><span class="line">pred(0) = 0 # saturate</span>
<span class="line">pred(1) = 0</span>
<span class="line">pred(2) = 1</span>
<span class="line">...</span></code></pre>

</figure>
<p><span>Now we can say:</span></p>

<figure class="code-block">


<pre><code><span class="line">sub(x, y) = LOOP(x, pred, y)</span>
<span class="line"></span>
<span class="line">and(x, y) = mul(x, y)</span>
<span class="line">or(x, y) = sub(</span>
<span class="line">  add(x, y),</span>
<span class="line">  mul(x, y)</span>
<span class="line">)</span>
<span class="line">not(x) = sub(1, x)</span>
<span class="line"></span>
<span class="line">if(cond, a, b) = add(</span>
<span class="line">  mul(a, cond),</span>
<span class="line">  mul(b, not(cond)),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>And now we can do a bunch of comparison operators:</span></p>

<figure class="code-block">


<pre><code><span class="line">is_zero(x) = sub(1, x)</span>
<span class="line"></span>
<span class="line"># x &gt;= y</span>
<span class="line">ge(x, y) = is_zero(sub(y, x))</span>
<span class="line"></span>
<span class="line"># x == y</span>
<span class="line">eq(x, y) = and(ge(x, y), ge(y, x))</span>
<span class="line"></span>
<span class="line"># x &gt; y</span>
<span class="line">gt(x, y) = and(ge(x, y), not(eq(x, y)))</span>
<span class="line"></span>
<span class="line"># x &lt; y</span>
<span class="line">lt(x, y) = gt(y, x)</span></code></pre>

</figure>
<p><span>With that we could implement modulus. To compute </span><code>x % m</code><span> we will start with </span><code>x</code><span>, and will be</span>
<span>subtracting </span><code>m</code><span> until we get a number smaller than </span><code>m</code><span>. We</span>&rsquo;<span>ll need at most </span><code>x</code><span> iterations for that.</span></p>
<p><span>In pseudo-code:</span></p>

<figure class="code-block">


<pre><code><span class="line"><span class="hl-keyword">def</span> <span class="hl-title function_">mod</span>(<span class="hl-params">x, m</span>):</span>
<span class="line">  current = x</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">for</span> _ <span class="hl-keyword">in</span> <span class="hl-number">0.</span>.x:</span>
<span class="line">    <span class="hl-keyword">if</span> current &lt; m:</span>
<span class="line">      current = current</span>
<span class="line">    <span class="hl-keyword">else</span>:</span>
<span class="line">      current = current - m</span>
<span class="line"></span>
<span class="line">  <span class="hl-keyword">return</span> current</span></code></pre>

</figure>
<p><span>And as a bona fide PRF:</span></p>

<figure class="code-block">


<pre><code><span class="line">mod_iter(m, x) = if(</span>
<span class="line">  lt(x, m),</span>
<span class="line">  x,        # then</span>
<span class="line">  sub(x, m) # else</span>
<span class="line">)</span>
<span class="line">mod(x, m) = LOOP(x, mod_iter m, x)</span></code></pre>

</figure>
<p><span>That</span>&rsquo;<span>s a curious structure </span>&mdash;<span> rather than computing the modulo directly, we essentially search for</span>
<span>it using trial and error, and relying on the fact that the search has a clear upper bound.</span></p>
<p><span>Division can be done similarly: to divide x by y, start with 0, and then repeatedly add one to the</span>
<span>accumulator until the product of the accumulator and y exceeds x:</span></p>

<figure class="code-block">


<pre><code><span class="line">div_iter x y acc = if(</span>
<span class="line">  le(mul(succ(acc), y), y),</span>
<span class="line">  succ(acc), # then</span>
<span class="line">  acc        # else</span>
<span class="line">)</span>
<span class="line">div(x, y) = LOOP(0, div_iter x y, x)</span></code></pre>

</figure>
<p><span>This really starts to look like programming! One thing we are currently missing are data structures.</span>
<span>While our functions take multiple arguments, they only return one number. But it</span>&rsquo;<span>s easy enough to</span>
<span>pack two numbers into one: to represent an </span><code>(a, b)</code><span> pair, we</span>&rsquo;<span>ll use 2</span><sup><span>a</span></sup><span> 3</span><sup><span>b</span></sup><span> number:</span></p>

<figure class="code-block">


<pre><code><span class="line">mk_pair(a, b) = mul(pow(2, a), pow(3, b))</span></code></pre>

</figure>
<p><span>To deconstruct such a pair into its first and second components, we need to find the maximum power</span>
<span>of 2 or 3 that divides our number. Which is exactly the same shape we used to implement </span><code>div</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line">max_factor_iter p m acc = if(</span>
<span class="line">  is_zero(mod(p, pow(m, succ(acc)))),</span>
<span class="line">  succ(acc), # then</span>
<span class="line">  acc,       # else</span>
<span class="line">)</span>
<span class="line">max_factor(p, m) = LOOP(0, max_factor_iter p m, p)</span>
<span class="line"></span>
<span class="line">fst(p) = max_factor(p, 2)</span>
<span class="line">snd(p) = max_factor(p, 3)</span></code></pre>

</figure>
<p><span>Here again we use the fact that the maximal power of two that divides </span><code>p</code><span> is not larger than </span><code>p</code>
<span>itself, so we can over-estimate the number of iterations we</span>&rsquo;<span>ll need as </span><code>p</code><span>.</span></p>
<p><span>Using this pair construction, we can finally add a loop counter to our </span><code>LOOP</code><span> construct. To track</span>
<span>the counter, we pack it as a pair with the accumulator:</span></p>

<figure class="code-block">


<pre><code><span class="line">LOOP(mk_pair(init, 0), f, n)</span></code></pre>

</figure>
<p><span>And then inside f, we first unpack that pair into accumulator and counter, pass them to actual loop</span>
<span>iteration, and then pack the result again, incrementing the counter:</span></p>

<figure class="code-block">


<pre><code><span class="line">f acc = mk_pair(</span>
<span class="line">  g(fst(acc), snd(acc)),</span>
<span class="line">  succ(snd(acc)),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>Ok, so we have achieved something remarkable: while we are writing terminating-by-construction</span>
<span>programs, which are definitely not Turing complete, we have constructed basic programming staples,</span>
<span>like boolean logic and data structures, and we have also built some rather complicated mathematical</span>
<span>functions, like </span><span class="display"><span>2</span><sup><span>2</span><sup><span>N</span></sup></sup><span>.</span></span></p>
<p><span>We could try to further enrich our little primitive recursive kingdom by adding more and more</span>
<span>functions on an ad hoc basis, but let</span>&rsquo;<span>s try to be really ambitious and go for the main prize </span>&mdash;
<span>simulating Turing Machines.</span></p>
<p><span>We know that we will fail: Turing machines can enter an infinite loop, but PRFs necessarily terminate.</span>
<span>That means, that, if a PRF were able to simulate an arbitrary TM, it would have to say after a certain</span>
<span>finite amount of steps that </span>&ldquo;<span>this TM doesn</span>&rsquo;<span>t terminate</span>&rdquo;<span>.  And, while we didn</span>&rsquo;<span>t do this, it</span>&rsquo;<span>s easy to</span>
<span>see that you </span><em><span>could</span></em><span> simulate the other way around and implement PRFs in a TM. But that would give</span>
<span>us a TM algorithm to decide if an arbitrary TM halts, which we know doesn</span>&rsquo;<span>t exist.</span></p>
<p><span>So, this is hopeless! But we might still be able to learn something from failing.</span></p>
<p><span>Ok! So let</span>&rsquo;<span>s start with a configuration of a TM which we somehow need to encode into a single</span>
<span>number. First, we need the state variable proper (Q0, Q1, etc), which seems easy enough to represent</span>
<span>with a number. Then, we need a tape and a position of the reading head. Recall how we used a pair of</span>
<span>stacks to represent exactly the tape and the position. And recall that we can look at a stack of</span>
<span>zeros and ones as a number in binary form, where push and pop operations are implemented using </span><code>%</code><span>,</span>
<code>*</code><span>, and </span><code>/</code><span> </span>&mdash;<span> exactly the operations we already can do. So, our configuration is just three</span>
<span>numbers: </span><span class="display"><code>(S, stack1, stack2)</code><span>.</span></span></p>
<p><span>And, using the 2</span><sup><span>a</span></sup><span>3</span><sup><span>b</span></sup><span>5</span><sup><span>c</span></sup><span> trick, we can pack this triple into just a single number. But that means we</span>
<span>could directly encode a single step of a Turing Machine:</span></p>

<figure class="code-block">


<pre><code><span class="line">single_step(config) = if(</span>
<span class="line">  # if the state is Q0 ...</span>
<span class="line">  eq(fst(config), 0)</span>
<span class="line"></span>
<span class="line">  # and the symbol at the top of left stack is 0</span>
<span class="line">  if(is_zero(mod(snd(config), 2))</span>
<span class="line">    mk_triple(</span>
<span class="line">      1,                    # move to state Q1</span>
<span class="line">      div(snd(config), 2),  # pop value from the left stack</span>
<span class="line">      mul(trd(config), 2),  # push zero onto the right stack</span>
<span class="line">    ),</span>
<span class="line">    ... # Handle symbol 1 in state Q1</span>
<span class="line">  )</span>
<span class="line">  # if the state is Q1 ...</span>
<span class="line">  if(eq(fst(config), 1)</span>
<span class="line">    ...</span>
<span class="line">  )</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>And now we could plug that into our </span><code>LOOP</code><span> to simulate a Turing Machine running for N steps:</span></p>

<figure class="code-block">


<pre><code><span class="line">n_steps initial_config n =</span>
<span class="line">  LOOP(initial_config, single_step, n)</span></code></pre>

</figure>
<p><span>The catch of course is that we can</span>&rsquo;<span>t know the </span><code>N</code><span> that</span>&rsquo;<span>s going to be enough. But we can have a very</span>
<span>good guess! We could do something like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">hopefully_enough_steps initial_config =</span>
<span class="line">  LOOP(initial_config, single_step, pow_2_2(initial_config))</span></code></pre>

</figure>
<p><span>That is, run for some large tower of exponents of the initial state. Which would be plenty for</span>
<span>normal algorithms, which are usually 2</span><sup><span>N</span></sup><span> at worst!</span></p>
<p><span>Or, generalizing:</span></p>

<figure class="blockquote">
<blockquote><p><span>If a TM has a runtime which is bounded by some primitive-recursive function, then the entire</span>
<span>TM can be replaced with a PRF. Be advised that PRFs can grow </span><em><span>really</span></em><span> fast.</span></p>
</blockquote>

</figure>
<p><span>Which is the headline result we have set out to prove!</span></p>
</section>
<section id="Primitive-Recursive-Functions-Limit">

    <h2>
    <a href="#Primitive-Recursive-Functions-Limit"><span>Primitive Recursive Functions: Limit</span> </a>
    </h2>
<p><span>It might seem that non-termination is the only principle obstacle. That anything that terminates at</span>
<span>all has to be implementable as a PRF. Alas, that</span>&rsquo;<span>s not so. Let</span>&rsquo;<span>s go and construct a function that is</span>
<span>surmountable by a TM, but is out of reach of PRFs.</span></p>
<p><span>We will combine the ideas of the impossibility proofs for FSMs (noting that if a function is</span>
<span>computed by some machine, that machine has a specific finite size) and TMs (diagonalization).</span></p>
<p><span>So, suppose we have some function </span><code>f</code><span> that can</span>&rsquo;<span>t be computed by a PRF. How would we go about proving</span>
<span>that? Well, we</span>&rsquo;<span>d start with </span>&ldquo;<span>suppose that we have a PRF P that computes </span><code>f</code>&rdquo;<span>. And then we could</span>
<span>notice that P would have some finite size. If you look at it abstractly, the P is its syntax tree,</span>
<span>with lots of </span><code>LOOP</code><span> constructs, but it always boils down to some </span><code>succ</code><span>s and </span><code>zero</code><span>s at the leaves.</span>
<span>Let</span>&rsquo;<span>s say that the depth of P is </span><code>d</code><span>.</span></p>
<p><span>And, actually, if you look at it, there are only a finite number of PRFs with depth at most </span><code>d</code><span>. Some</span>
<span>of them describe pretty fast growing functions. But probably there</span>&rsquo;<span>s a limit to how fast a function</span>
<span>can grow, given that it is computed by a PRF of size </span><code>d</code><span>. Or, to use a concrete example: we have</span>
<span>constructed a PRF of depth 5 that computes two to the power of two to the power of N. Probably if we</span>
<span>were smarter, we could have squeezed a couple more levels into that tower of exponents. But</span>
<span>intuitively it seems that if you build a tower of, say, 10 exponents, that would grow faster than</span>
<em><span>any</span></em><span> PRF of depth </span><code>5</code><span>. And that this generalizes </span>&mdash;<span> for any fixed depth, there</span>&rsquo;<span>s a high-enough</span>
<span>tower of exponents that grows faster than any PRF with that depth.</span></p>
<p><span>So we could conceivably build an </span><code>f</code><span> that defeats our </span><code>d</code><span>-deep P. But that</span>&rsquo;<span>s not quite a victory</span>
<span>yet: maybe that </span><code>f</code><span> is feasible for </span><code>d+2</code><span>-deep PRFs! So here we</span>&rsquo;<span>ll additionally apply</span>
<span>diagonalization: for each depth, we</span>&rsquo;<span>ll build it</span>&rsquo;<span>s own depth-specific nemesis </span><code>f_d</code><span>. And then we</span>&rsquo;<span>ll</span>
<span>define our overall function as</span></p>

<figure class="code-block">


<pre><code><span class="line">a(n) = f_n(n)</span></code></pre>

</figure>
<p><span>So, for </span><code>n</code><span> large enough it</span>&rsquo;<span>ll grow faster than a PRF with any fixed depth.</span></p>
<p><span>So that</span>&rsquo;<span>s the general plan, the rest of the own is basically just calculating the upper bound on the</span>
<span>growth of a PRF of depth </span><code>d</code><span>.</span></p>
<p><span>One technical difficulty here is that PRFs tend to have different arities:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y)</span>
<span class="line">g(x, y, z, t)</span>
<span class="line">h(x)</span></code></pre>

</figure>
<p><span>Ideally, we</span>&rsquo;<span>d use just one upper bound of them all. So we</span>&rsquo;<span>ll be looking for an upper bound of the</span>
<span>following form:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y, z, t) &lt;= A_d(max(x, y, z, t))</span></code></pre>

</figure>
<p><span>That is:</span></p>
<ul>
<li>
<span>Compute the depth of </span><code>f</code><span>, </span><code>d</code><span>.</span>
</li>
<li>
<span>Compute the largest of its arguments.</span>
</li>
<li>
<span>And plug that into unary function for depth </span><code>d</code><span>.</span>
</li>
</ul>
<p><span>Let</span>&rsquo;<span>s start with </span><code>d=1</code><span>. We have only primitive functions on this level, </span><code>succ</code><span>, </span><code>zero</code><span>, and </span><code>pred</code><span>,</span>
<span>so we could say that</span></p>

<figure class="code-block">


<pre><code><span class="line">A_1(x) = x + 1</span></code></pre>

</figure>
<p><span>Now, let</span>&rsquo;<span>s handle an arbitrary other depth </span><code>d + 1</code><span>. In that case, our function is non-primitive, so at</span>
<span>the root of the syntax tree we have either a composition or a </span><code>LOOP</code><span>.</span></p>
<p><span>Composition would look like this:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y, z, ...) = g(</span>
<span class="line">  h1(x, y, z, ...),</span>
<span class="line">  h2(x, y, z, ...),</span>
<span class="line">  h3(x, y, z, ...),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>where </span><code>g</code><span> and </span><code>h_n</code><span> are </span><code>d</code><span> deep and the resulting </span><code>f</code><span> is </span><code>d+1</code><span> deep. We can immediately estimate</span>
<span>the </span><code>h_n</code><span> then:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) &lt;= g(</span>
<span class="line">  A_d(maxarg),</span>
<span class="line">  A_d(maxarg),</span>
<span class="line">  A_d(maxarg),</span>
<span class="line">  ...</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>In this somewhat loose notation, </span><code>args...</code><span> stands for a tuple of arguments, and </span><code>maxarg</code><span> stands for</span>
<span>the largest one.</span></p>
<p><span>And then we could use the same estimate for </span><code>g</code><span>:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) &lt;= A_d(A_d(maxarg))</span></code></pre>

</figure>
<p><span>This is super high-order, so let</span>&rsquo;<span>s do a concrete example for a depth-2 two-argument function which</span>
<span>starts with a composition:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y) &lt;= A_1(A_1(max(x, y)))</span>
<span class="line">         = A_1(max(x, y) + 1)</span>
<span class="line">         = max(x, y) + 2</span></code></pre>

</figure>
<p><span>This sounds legit: if we don</span>&rsquo;<span>t use LOOP, then </span><code>f(x, y)</code><span> is either </span><code>succ(succ(x))</code><span> or </span><code>succ(succ(y))</code>
<span>so </span><code>max(x, y) + 2</code><span> indeed is the bound!</span></p>
<p><span>Ok, now the fun case! If the top-level node is a </span><code>LOOP</code><span>, then we have</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) = LOOP(</span>
<span class="line">  g(args...),</span>
<span class="line">  h(args...),</span>
<span class="line">  t(args...),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>This sounds complicated to estimate, especially due to that last </span><code>t(args...)</code><span> argument, which is the</span>
<span>number of iterations. So we</span>&rsquo;<span>ll be cowards and </span><em><span>won</span>&rsquo;<span>t</span></em><span> actually try to estimate this case. Instead,</span>
<span>we will require that our PRF is written in a simplified form, where the first and the last arguments</span>
<span>to </span><code>LOOP</code><span> are simple.</span></p>
<p><span>So, if your PRF looks like</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y) = LOOP(x + y, mul, pow2(x))</span></code></pre>

</figure>
<p><span>you are required to re-write it first as</span></p>

<figure class="code-block">


<pre><code><span class="line">helper(u, v) = LOOP(u, mul, v)</span>
<span class="line">f(x, y) = helper(x + y, pow2(x))</span></code></pre>

</figure>
<p><span>So now we only have to deal with this:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) = LOOP(</span>
<span class="line">  arg,</span>
<span class="line">  g(args...),</span>
<span class="line">  arg,</span>
<span class="line">)</span></code></pre>

</figure>
<p><code>f</code><span> has depth </span><code>d+1</code><span>, </span><code>g</code><span> has depth </span><code>d</code><span>.</span></p>
<p><span>On the first iteration, we</span>&rsquo;<span>ll call </span><code>g(args..., arg)</code><span>, which we can estimate as </span><code>A_d(maxarg)</code><span>. That</span>
<span>is, </span><code>g</code><span> does get an </span><em><span>extra</span></em><span> argument, but it is one of the original arguments of </span><code>f</code><span>, and we are</span>
<span>looking at the maximum argument anyway, so it doesn</span>&rsquo;<span>t matter.</span></p>
<p><span>On the second iteration, we are going to call</span>
<code class="display">g(args..., prev_iteration)</code>
<span>which we can estimate as</span>
<span class="display"><code>A_d(max(maxarg, prev_iteration))</code><span>.</span></span></p>
<p><span>Now we plug our estimation for the first iteration:</span></p>

<figure class="code-block">


<pre><code><span class="line">g(args..., prev_iteration)</span>
<span class="line">  &lt;= A_d(max(maxarg, prev_iteration))</span>
<span class="line">  &lt;= A_d(max(maxarg, A_d(maxarg)))</span>
<span class="line">  =  A_d(A_d(maxarg))</span></code></pre>

</figure>
<p><span>That is, the estimate for the first iteration is </span><code>A_d(maxarg)</code><span>. The estimation for the second</span>
<span>iteration adds one more layer: </span><code>A_d(A_d(maxarg))</code><span>. For the third iteration we</span>&rsquo;<span>ll get</span>
<span class="display"><code>A_d(A_d(A_d(maxarg)))</code><span>.</span></span></p>
<p><span>So the overall thing is going to be smaller than </span><code>A_d</code><span> iteratively applied to itself some number of</span>
<span>times, where </span>&ldquo;<span>some number</span>&rdquo;<span> is one of the </span><code>f</code><span> original arguments. But no harm</span>&rsquo;<span>s done if we iterate up</span>
<span>to </span><code>maxarg</code><span>.</span></p>
<p><span>As a sanity check, the worst depth-2 function constructed with iteration is probably</span></p>

<figure class="code-block">


<pre><code><span class="line">f(x, y) = LOOP(x, succ, y)</span></code></pre>

</figure>
<p><span>which is </span><code>x + y</code><span>. And our estimate gives </span><code>x + 1</code><span> applied </span><code>maxarg</code><span> times to </span><code>maxarg</code><span>, which is </span><code>2 *
maxarg</code><span>, which is indeed the correct upper bound!</span></p>
<p><span>Combining everything together, we have:</span></p>

<figure class="code-block">


<pre><code><span class="line">A_1(x) = x + 1</span>
<span class="line"></span>
<span class="line">f(args...) &lt;= max(</span>
<span class="line">  A_d(A_d(maxarg)),               # composition case</span>
<span class="line">  A_d(A_d(A_d(... A_d(maxarg)))), # LOOP case,</span>
<span class="line">   &lt;-    maxarg A's         -&gt;</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>That </span><code>max</code><span> there is significant </span>&mdash;<span> although it seems like the second line, with </span><code>maxarg</code>
<span>applications, is always going to be longer, </span><code>maxarg</code><span>, in fact, could be as small as zero. But we</span>
<span>can take </span><code>maxarg + 2</code><span> repetitions to fix this:</span></p>

<figure class="code-block">


<pre><code><span class="line">f(args...) &lt;=</span>
<span class="line">  A_d(A_d(A_d(... A_d(maxarg)))),</span>
<span class="line">  &lt;-    maxarg + 2 A's         -&gt;</span></code></pre>

</figure>
<p><span>So let</span>&rsquo;<span>s just define </span><code>A_{d+1}(x)</code><span> to make that inequality work:</span></p>

<figure class="code-block">


<pre><code><span class="line">A_{d+1}(x) = A_d(A_d( .... A_d(x)))</span>
<span class="line">            &lt;- x + 2 A_d's in total-&gt;</span></code></pre>

</figure>
<p><span>Unpacking:</span></p>
<p><span>We define a family of unary functions </span><code>A_d</code><span>, such that each </span><code>A_d</code><span> </span>&ldquo;<span>grows faster</span>&rdquo;<span> than any n-ary PRF</span>
<span>of depth </span><code>d</code><span>. If </span><code>f</code><span> is a ternary PRF of depth 3, then </span><span class="display"><code>f(1, 92, 10) &lt;= A_3(92)</code><span>.</span></span></p>
<p><span>To evaluate </span><code>A_d</code><span> at point </span><code>x</code><span>, we use the following recursive procedure:</span></p>
<ul>
<li>
<span>If </span><code>d</code><span> is </span><code>1</code><span>, return </span><code>x + 1</code><span>.</span>
</li>
<li>
<span>Otherwise, evaluate </span><code>A_{d-1}</code><span> at point </span><code>x</code><span> to get, say, </span><code>v</code><span>. Then evaluate </span><code>A_{d-1}</code><span> again at</span>
<span>point </span><code>v</code><span> this time, yielding </span><code>u</code><span>. Then compute </span><code>A_{d-1}(u)</code><span>. Overall, repeat this process </span><code>x+2</code>
<span>times, and return the final number.</span>
</li>
</ul>
<p><span>We can simplify this a bit if we stop treating </span><code>d</code><span> as a kind of function </span><em><span>index</span></em><span>, and instead say</span>
<span>that our </span><code>A</code><span> is just a function of two arguments. Then we have the following equations:</span></p>

<figure class="code-block">


<pre><code><span class="line">A(1, x) = x + 1</span>
<span class="line">A(d + 1, x) = A(d, A(d, A(d, ..., A(d, x))))</span>
<span class="line">                &lt;- x + 2 A_d's in total-&gt;</span></code></pre>

</figure>
<p><span>The last equation can re-formatted as</span></p>

<figure class="code-block">


<pre><code><span class="line">A(</span>
<span class="line">  d,</span>
<span class="line">  A(d, A(d, ..., A(d, x))),</span>
<span class="line">  &lt;- x + 1 A_d's in total-&gt;</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>And for non-zero x that is just</span></p>

<figure class="code-block">


<pre><code><span class="line">A(</span>
<span class="line">  d,</span>
<span class="line">  A(d + 1, x - 1),</span>
<span class="line">)</span></code></pre>

</figure>
<p><span>So we get the following recursive definition for A(d, x):</span></p>

<figure class="code-block">


<pre><code><span class="line">A(1, x) = x + 1</span>
<span class="line">A(d + 1, 0) = A(d, A(d, 0))</span>
<span class="line">A(d + 1, x) = A(d, A(d + 1, x - 1))</span></code></pre>

</figure>
<p><span>As a Python program:</span></p>

<figure class="code-block">


<pre><code><span class="line">def A(d, x):</span>
<span class="line">  if d == 1: return x + 1</span>
<span class="line">  if x == 0: return A(d-1, A(d-1, 0))</span>
<span class="line">  return A(d-1, A(d, x - 1))</span></code></pre>

</figure>
<p><span>It</span>&rsquo;<span>s easy to see that computing </span><code>A</code><span> on a Turing Machine using this definition terminates </span>&mdash;<span> this</span>
<span>is a function with two arguments, and every recursive call uses a lexicographically smaller pair of</span>
<span>arguments. And we constructed A in such a way that </span><code>A(d, x)</code><span> as a function of </span><code>x</code><span> is larger than any</span>
<span>PRF with a single argument of depth d. But that means that the following function with one argument</span>
<code class="display">a(x) = A(x, x) </code></p>
<p><span>grows faster than </span><em><span>any</span></em><span> PRF. And that</span>&rsquo;<span>s an example of a function which a Turing Machine has no</span>
<span>trouble computing (given sufficient time), but which is beyond the capabilities of PRFs.</span></p>
</section>
<section id="Part-III-Descent-From-the-Ivory-Tower">

    <h2>
    <a href="#Part-III-Descent-From-the-Ivory-Tower"><span>Part III, Descent From the Ivory Tower</span> </a>
    </h2>
<p><span>Remember, this is a three-part post! And are finally at the part 3! So let</span>&rsquo;<span>s circle back to the</span>
<span>practical matters. We have learned that:</span></p>
<ul>
<li>
<span>Turing machines don</span>&rsquo;<span>t necessarily terminate.</span>
</li>
<li>
<span>While other computational devices, like FSMs and PRFs, can be made to always terminate, there</span>&rsquo;<span>s no</span>
<span>guarantee that they</span>&rsquo;<span>ll terminate fast. PRFs in particular can compute quite large functions!</span>
</li>
<li>
<span>And non-Turing complete devices can be quite expressive. For example, any real-world algorithm</span>
<span>that works on a TM can be adapted to run as a PRF.</span>
</li>
<li>
<span>Moreover, you don</span>&rsquo;<span>t even have to contort the algorithm much to make it fit. There</span>&rsquo;<span>s a universal</span>
<span>recipe for how to take something Turing complete and make it a primitive recursive function</span>
<span>instead </span>&mdash;<span> just add an iteration counter to the device, and forcibly halt it if the counter grows</span>
<span>too large.</span>
</li>
</ul>
<p><span>Or, more succinctly: there</span>&rsquo;<span>s no practical difference between a program that doesn</span>&rsquo;<span>t terminate, and</span>
<span>the one that terminates after a billion years. As a practitioner, if you think you need to solve the</span>
<span>first problem, you need to solve the second problem as well. And making your programming language</span>
<span>non-Turing complete doesn</span>&rsquo;<span>t really help with this.</span></p>
<p><span>And yet, there are a lot of configuration languages out there that use non-Turing completeness as</span>
<span>one of their key design goals. Why is that?</span></p>
<p><span>I would say that we are never interested in Turing-completeness per-se. We usually want some </span><em><span>much</span></em>
<span>stronger properties. And yet there</span>&rsquo;<span>s no convenient catchy name for that bag of features of a good</span>
<span>configuration language. So, </span>&ldquo;<span>non-Turing-complete</span>&rdquo;<span> gets used as a sort of rallying cry to signal that</span>
<span>something is a good configuration language, and maybe sometimes even to justify to others inventing</span>
<span>a new language instead of taking something like Lua. That is, the </span><em><span>real</span></em><span> reason why you want at</span>
<span>least a different implementation is all those properties you really need, but they are kinda hard to</span>
<span>explain, or at least much harder than </span>&ldquo;<span>we can</span>&rsquo;<span>t use Python/Lua/JavaScript because they are</span>
<span>Turing-complete</span>&rdquo;<span>.</span></p>
<p><span>So what </span><em><span>are</span></em><span> the properties of a good configuration language?</span></p>
<p><em><span>First</span></em><span>, we need the language to be deterministic. If you launch Python and type </span><code>id([])</code><span>, you</span>&rsquo;<span>ll</span>
<span>see some number. If you hit </span><code>^C</code><span>, and than do this again, you</span>&rsquo;<span>ll see a different number. This is OK</span>
<span>for </span>&ldquo;<span>normal</span>&rdquo;<span> programming, but is usually anathema for configuration. Configuration is often used as a</span>
<span>key in some incremental, caching system, and letting in non-determinism there wreaks absolute chaos!</span></p>
<p><em><span>Second</span></em><span>, you need the language to be well-defined. You can compile Python with ASLR disabled, and</span>
<span>use some specific allocator, such that </span><code>id([])</code><span> always returns the same result. But that result</span>
<span>would be hard to predict! And if someone tries to do an alternative implementation, even if they</span>
<span>disable ASLR as well, they are likely to get a different deterministic number! Or the same could</span>
<span>happen if you just update the version of Python. So, the semantics of the language should be clearly</span>
<span>pinned-down by some sort of a reference, such that it is possible to guarantee not only</span>
<span>deterministic behavior, but fully identical behavior across different implementations.</span></p>
<p><em><span>Third</span></em><span>, you need the language to be pure. If your configuration can access environment variables or</span>
<span>read files on disk, than the meaning of the configuration would depend on the environment where the</span>
<span>configuration is evaluated, and you again don</span>&rsquo;<span>t want that, to make caching work.</span></p>
<p><em><span>Fourth</span></em><span>, a thing that is closely related to purity is security and sandboxing. The </span><em><span>mechanism</span></em><span> to</span>
<span>achieve both purity and security is the same </span>&mdash;<span> you don</span>&rsquo;<span>t expose general IO to your language. But</span>
<span>the purpose is different: purity is about not letting the results be non-deterministic, while</span>
<span>security is about not exposing access tokens to the attacker.</span></p>
<p><span>And now this gets tricky. One particular possible attack is a denial of service </span>&mdash;<span> sending some bad</span>
<span>config which makes our system just spin there burning the CPU. Even if you control all IO, you</span>
<span>are generally still open to these kinds of attacks. It might be OK to say this is outside of the</span>
<span>threat model </span>&mdash;<span> that no one would find it valuable enough to just burn your CPU, if they can</span>&rsquo;<span>t also</span>
<span>do IO, and that, even in the event that this happens, there</span>&rsquo;<span>s going to be some easy mitigation in the</span>
<span>form of a higher-level timeout.</span></p>
<p><span>But you also might choose to provide some sort of guarantees about execution time, and that</span>&rsquo;<span>s really</span>
<span>hard. Two approaches work. One is to make sure that processing is </span><em><span>obviously linear</span></em><span>. Not just</span>
<span>terminates, but is actually proportional to the size of inputs, and in a very direct way. If the</span>
<span>correspondence is not direct, than it</span>&rsquo;<span>s highly likely that it is in fact non linear. The second</span>
<span>approach is to ensure </span><em><span>metered execution</span></em><span> </span>&mdash;<span> during processing, decrement a counter for every</span>
<span>simple atomic step and terminate processing when the counter reaches zero.</span></p>
<p><em><span>Finally</span></em><span> one more vague property you</span>&rsquo;<span>d want from a configuration language is for it to be simple.</span>
<span>That is, to ensure that, when people use your language, they write simple programs. It seems to me</span>
<span>that this might actually be the case where banning recursion and unbounded loops could help, though</span>
<span>I am not sure. As we know from the PRF exercise, this won</span>&rsquo;<span>t actually prevent people from writing</span>
<span>arbitrary recursive programs. It</span>&rsquo;<span>ll just require </span><a href="https://mochiro.moe/posts/09-meson-raytracer/"><span>some roundabout</span>
<span>code</span></a><span> to do that. But maybe that</span>&rsquo;<span>ll be enough of a</span>
<span>speedbump to make someone invent a simple solution, instead of brute-forcing the most obvious one?</span></p>
<p><span>That</span>&rsquo;<span>s all for today! Have a great weekend, and remember:</span></p>

<figure class="blockquote">
<blockquote><p><span>Any algorithm that can be implemented by a Turing Machine such that its runtime is bounded by some</span>
<span>primitive recursive function of input can also be implemented by a primitive recursive function!</span></p>
</blockquote>

</figure>
</section>
]]></content>
</entry>

</feed>
